{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1ec27b-302c-4628-9174-5dc08e97da34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcasted shape\n",
    "def broadcasted_shape(shape_X, shape_Y):\n",
    "\n",
    "    max_len = max(len(shape_X), len(shape_Y))\n",
    "    min_len = min(len(shape_X), len(shape_Y))\n",
    "    swapped = False\n",
    "\n",
    "    if len(shape_X) < max_len:\n",
    "        swapped = True\n",
    "        L, S = shape_Y, shape_X # L = long, S = short\n",
    "    else:\n",
    "        L, S = shape_X, shape_Y\n",
    "\n",
    "    L_rev = L[::-1]\n",
    "    S_rev = S[::-1]\n",
    "\n",
    "    result_shape = []\n",
    "    axes_L_expanded = []\n",
    "    axes_S_expanded = []\n",
    "\n",
    "    for i in range(min_len):\n",
    "        dim_L = L_rev[i]\n",
    "        dim_S = S_rev[i]\n",
    "        if dim_L == 1 and dim_S != 1:\n",
    "            axes_L_expanded.append(max_len  -1- i)\n",
    "        elif dim_L != 1 and dim_S == 1:\n",
    "            axes_S_expanded.append(max_len  -1- i)\n",
    "        if dim_L == 1 or dim_S == 1 or dim_L == dim_S:\n",
    "            result_shape.append(max(dim_L,dim_S))\n",
    "        else:\n",
    "            raise ValueError(f\"Shapes {shape_X} and {shape_Y} not broadcastable\")\n",
    "\n",
    "\n",
    "    result_shape += L_rev[(min_len):]\n",
    "\n",
    "    result_shape = tuple(result_shape[::-1])\n",
    "    axes_L_expanded = tuple(axes_L_expanded[::-1])\n",
    "    axes_S_expanded = tuple(axes_S_expanded[::-1])\n",
    "\n",
    "    if swapped:\n",
    "        return result_shape, axes_S_expanded, max_len - min_len,  axes_L_expanded, 0\n",
    "    else:\n",
    "        return result_shape,  axes_L_expanded, 0, axes_S_expanded, max_len - min_len\n",
    "\n",
    "def unbroadcast(arr, ax, pad):\n",
    "    return np.sum(np.sum(arr, axis = ax, keepdims = True), axis = tuple(range(pad)))\n",
    "class ag: # AutoGrad\n",
    "\n",
    "    #################\n",
    "    # ENTRYWISE OPS #\n",
    "    #################\n",
    "    def log(input):\n",
    "        output = ag.Tensor(np.log(input.value), inputs=[input], op=\"log\")\n",
    "        def _backward():\n",
    "            input.grad += output.grad / input.value\n",
    "            return None\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "    def exp(input):\n",
    "\n",
    "        output = ag.Tensor(np.exp(input.value), inputs=[input], op=\"exp\")\n",
    "\n",
    "        def _backward():\n",
    "            input.grad += output.grad * output.value\n",
    "            return None\n",
    "\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "    def relu(input):\n",
    "        output = ag.Tensor(np.maximum(0, input.value), inputs=[input], op=\"relu\")\n",
    "\n",
    "        def _backward():\n",
    "            input.grad += (input.value > 0)*output.grad\n",
    "            return None\n",
    "\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "    #################\n",
    "    # REDUCTIVE OPS #\n",
    "    #################\n",
    "    def sum(input,axis = None, keepdims = False):\n",
    "        output = ag.Tensor(np.sum(input.value, axis = axis, keepdims = keepdims), inputs = [input], op='sum')\n",
    "        def _backward():\n",
    "            if axis == None or keepdims:\n",
    "                input.grad += output.grad\n",
    "            else:\n",
    "                input.grad += np.expand_dims(output.grad, axis = axis)\n",
    "            return None\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "    def matmul(input1, input2):\n",
    "        return input1@input2\n",
    "    ###############\n",
    "    # SHAPING OPS #\n",
    "    ###############\n",
    "\n",
    "    def expand_dims(input, axis):\n",
    "        output = ag.Tensor(np.expand_dims(input.value,axis=axis), inputs = [input])\n",
    "        def _backward():\n",
    "            input.grad += np.squeeze(output.grad, axis = axis)\n",
    "            return None\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "    def reshape(input, newshape):\n",
    "        output = ag.Tensor(np.reshape(input.value, newshape), inputs=[input], op=\"reshape\")\n",
    "        def _backward():\n",
    "            input.grad += np.reshape(output.grad, input.shape)\n",
    "            return None\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "    def moveaxis(input, source, destination):\n",
    "        output = ag.Tensor(np.moveaxis(input.value, source, destination), inputs=[input], op=\"moveaxis\")\n",
    "\n",
    "        def _backward():\n",
    "            input.grad += np.moveaxis(output.grad, source, destination)\n",
    "            return None\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "\n",
    "    class Tensor: # Tensor with grads\n",
    "        def __init__(self,\n",
    "                     value,\n",
    "                     op=\"\",\n",
    "                     _backward= lambda : None,\n",
    "                     inputs=[],\n",
    "                     label=\"\",\n",
    "                     requires_grad=True):\n",
    "\n",
    "            if type(value) in [float ,int]:\n",
    "                value = np.array(value).astype(np.float64)\n",
    "            self.value = value.astype(np.float64)\n",
    "            self.grad = np.zeros_like(self.value).astype(np.float64)\n",
    "\n",
    "            self.shape = value.shape\n",
    "\n",
    "            self._backward = _backward\n",
    "            self.inputs = inputs\n",
    "\n",
    "            self.op = op\n",
    "            self.label = label\n",
    "\n",
    "            self.requires_grad = requires_grad\n",
    "\n",
    "        def topological_sort(self):\n",
    "            topo_order = []\n",
    "            visited = set()\n",
    "\n",
    "            def dfs(node):\n",
    "                if node not in visited:\n",
    "                    visited.add(node)\n",
    "                    for input in node.inputs:\n",
    "                        dfs(input)\n",
    "                    topo_order.append(node)\n",
    "\n",
    "            dfs(self)\n",
    "            return topo_order\n",
    "\n",
    "        def backward(self):\n",
    "            self.grad = np.array(1.0).astype(np.float64)\n",
    "\n",
    "            topo_order = self.topological_sort()\n",
    "\n",
    "            for node in reversed(topo_order):\n",
    "                node._backward()\n",
    "\n",
    "\n",
    "\n",
    "        def __add__(self, other):\n",
    "\n",
    "            if type(other) in [float, int]:\n",
    "                other = ag.Tensor(1.0*other)\n",
    "            result_shape, ax1, pad1, ax2, pad2 = broadcasted_shape(self.shape, other.shape)\n",
    "\n",
    "            output = ag.Tensor(self.value + other.value,\n",
    "                               inputs=[self, other], op=\"add\")\n",
    "            def _backward():\n",
    "                self.grad += unbroadcast(output.grad, ax1, pad1)\n",
    "                other.grad += unbroadcast(output.grad, ax2, pad2)\n",
    "\n",
    "            output._backward = _backward\n",
    "            return output\n",
    "\n",
    "        def __sub__(self,other):\n",
    "            return self + other*(-1)\n",
    "\n",
    "        def __neg__(self):\n",
    "            output = ag.Tensor(-self.value, inputs=[self], op=\"neg\")\n",
    "            def _backward():\n",
    "                self.grad -= output.grad\n",
    "                return None\n",
    "            output._backward = _backward\n",
    "            return output\n",
    "\n",
    "        def __mul__(self, other):\n",
    "            if type(other) in [float, int]:\n",
    "                other = ag.Tensor(1.0*other)\n",
    "            result_shape, ax1, pad1, ax2, pad2 = broadcasted_shape(self.shape, other.shape)\n",
    "\n",
    "            output = ag.Tensor(self.value * other.value,\n",
    "                               inputs=[self, other], op=\"mul\")\n",
    "            def _backward():\n",
    "                self.grad += unbroadcast(output.grad*other.value, ax1, pad1)\n",
    "                other.grad += unbroadcast(output.grad*self.value, ax2, pad2)\n",
    "\n",
    "            output._backward = _backward\n",
    "            return output\n",
    "\n",
    "        def __truediv__(self,other):\n",
    "            return self*(other**(-1))\n",
    "\n",
    "\n",
    "        def __pow__(self, exponent): # exponent is just a python float\n",
    "            output = ag.Tensor(self.value ** exponent,\n",
    "                               inputs=[self],\n",
    "                               op=f\"pow({exponent})\")\n",
    "\n",
    "            def _backward():\n",
    "\n",
    "                self.grad += (exponent * self.value**(exponent-1)) * output.grad\n",
    "                return None\n",
    "\n",
    "            output._backward = _backward\n",
    "            return output\n",
    "\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            output = ag.Tensor(np.array(self.value[idx]),\n",
    "                               inputs = [self],\n",
    "                               op=f\"[...]\")\n",
    "            def _backward():\n",
    "                self.grad[idx] += output.grad # idx must not have repeats!\n",
    "                return None\n",
    "            output._backward = _backward\n",
    "            return output\n",
    "\n",
    "        def __radd__(self, other):\n",
    "            return self + other\n",
    "\n",
    "        def __rmul__(self, other):\n",
    "            return self * other\n",
    "\n",
    "        def __rsub__(self, other):\n",
    "            return (-self) + other\n",
    "\n",
    "        def __rtruediv__(self, other):\n",
    "            return ag.Tensor(other) / self\n",
    "\n",
    "\n",
    "        def __matmul__(self,other):\n",
    "            output = ag.Tensor(np.matmul(self.value,other.value),\n",
    "                               inputs = [self,other],\n",
    "                               op=\"matmul\")\n",
    "\n",
    "            # self.shape = (a,b,c, m,n)\n",
    "            # other.shape = (a,b,c, n,p)\n",
    "            # output.grad.shape == output.value.shape = (a,b,c, m,p)\n",
    "\n",
    "\n",
    "            def _backward():\n",
    "                if len(other.value.shape) == 1:\n",
    "                    if len(self.value.shape) == 1:\n",
    "                        raise Exception(\"To take the dot-product of two vectors, use ag.sum(x*y) instead.\")\n",
    "                    else:\n",
    "                        self.grad += np.matmul(output.grad[:,None], other.value[None,:])\n",
    "                else:\n",
    "                    self.grad += np.matmul(output.grad, np.moveaxis(other.value,-1,-2))\n",
    "                axis = list(range(len(self.value.shape)-1))\n",
    "                other.grad += np.tensordot(self.value, output.grad, axes = [axis,axis])\n",
    "                return None\n",
    "            output._backward = _backward\n",
    "            return output\n",
    "\n",
    "        def reshape(self, *newshape):\n",
    "            output = ag.Tensor(np.reshape(self.value, tuple(newshape)), inputs=[self], op=\"reshape\")\n",
    "            def _backward():\n",
    "                self.grad += np.reshape(output.grad, self.shape)\n",
    "                return None\n",
    "            output._backward = _backward\n",
    "            return output\n",
    "\n",
    "\n",
    "\n",
    "        def __repr__(self) -> str:\n",
    "            return \"Value:\\n\"+self.value.__repr__() + \"\\nGrad:\\n\" + self.grad.__repr__()\n",
    "        # def __repr__(self) -> str:\n",
    "        #     return \"Value:\\n\"+self.value.__repr__() + \"\\nGrad:\\n\" + self.grad.__repr__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d01e59c-ac6a-4135-b26a-fd7379e6bd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class nn:\n",
    "    # A fake python class so that we can use nn as an \"namespace\"\n",
    "    # This way we can avoid having an actual python module\n",
    "    # which would create more files for you to have to download\n",
    "    # and for me to upload.\n",
    "\n",
    "    # But the sacrifice is that the class inheritance is a bit wonky\n",
    "    # Maybe someone more well-versed in python class inheritance can help me out\n",
    "    # Email me if you'd like to contribute to making this class better next time\n",
    "\n",
    "    class Module:\n",
    "        def __init__(self):\n",
    "            self._parameters = {}\n",
    "            self._modules = {}\n",
    "\n",
    "        def forward(self, *inputs):\n",
    "            raise NotImplementedError\n",
    "\n",
    "        def __call__(self, *inputs):\n",
    "            return self.forward(*inputs)\n",
    "\n",
    "        def parameters(self):\n",
    "            params = list(self._parameters.values())\n",
    "            for module in self._modules.values():\n",
    "                params.extend(module.parameters())\n",
    "            return params\n",
    "\n",
    "        def __setattr__(self, name, value):\n",
    "            object.__setattr__(self, name, value)\n",
    "            if isinstance(value, nn.Module):\n",
    "                self._modules[name] = value\n",
    "            elif isinstance(value, ag.Tensor):\n",
    "                self._parameters[name] = value\n",
    "\n",
    "\n",
    "    class Linear(Module):\n",
    "        def __init__(self, in_features, out_features):\n",
    "            nn.Module.__init__(self)\n",
    "            kaiming_he_init_constant = np.sqrt(2 / in_features)\n",
    "\n",
    "            self.weight = ag.Tensor(np.random.randn(in_features, out_features) * kaiming_he_init_constant )\n",
    "            self.bias = ag.Tensor(np.zeros(out_features))\n",
    "\n",
    "            # these do not trigger __setattr__\n",
    "            self._parameters['weight'] = self.weight\n",
    "            self._parameters['bias'] = self.bias\n",
    "\n",
    "        def forward(self, x):\n",
    "            return ag.matmul(x, self.weight) + self.bias\n",
    "\n",
    "\n",
    "    class MSELoss:\n",
    "        def __call__(self, input, target):\n",
    "            N = target.value.shape[0]\n",
    "            return ag.sum((target - input)**2) / N\n",
    "\n",
    "    class BinaryCrossEntropyLoss:\n",
    "        def __call__(self, input, target):\n",
    "            N = target.value.shape[0]\n",
    "            return ag.sum( ag.log(1.0+ ag.exp(-input*target))) / N\n",
    "\n",
    "    class CrossEntropyLoss:\n",
    "        def __call__(self, input, target):\n",
    "            # input should be an ag.Tensor of shape (N, num_classes)\n",
    "            # target should be a numpy array of shape (N,) of elements of [0,1,..., num_classes-1]\n",
    "\n",
    "            z = input\n",
    "            y = target\n",
    "\n",
    "            expz = ag.exp(z)\n",
    "\n",
    "            p = expz / ag.sum(expz, axis=-1, keepdims=True)\n",
    "            N = z.shape[0]\n",
    "            l = ag.sum(-ag.log(p[np.arange(N), y]))/N\n",
    "            return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01150eae-d352-47d4-8da9-e294d223404f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class optim:\n",
    "    class SGD:\n",
    "        def __init__(self, parameters, lr=0.01):\n",
    "            self.parameters = parameters\n",
    "            self.lr = lr\n",
    "\n",
    "        def step(self):\n",
    "            for p in self.parameters:\n",
    "                p.value -= self.lr * p.grad\n",
    "\n",
    "        def zero_grad(self):\n",
    "            for p in self.parameters:\n",
    "                p.grad = np.zeros_like(p.value)\n",
    "                \n",
    "    class SGDWithMomentum:\n",
    "        def __init__(self, parameters, lr=0.01, momentum=0.9):\n",
    "            # momentum is gamma\n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError\n",
    "\n",
    "        def step(self):\n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError\n",
    "\n",
    "        def zero_grad(self):\n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3b6c4d-5993-4351-97b3-866f6bfdc8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "x = np.random.randn(10, 3)\n",
    "w_true = np.random.randn(3)\n",
    "b = np.random.randn(1)\n",
    "eps = 0.1 * np.random.randn(10)\n",
    "y = x @ w_true + b + eps\n",
    "\n",
    "\n",
    "X = np.hstack([x, np.ones((x.shape[0], 1))])\n",
    "\n",
    "# Least squares solution\n",
    "w_b_hat = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "\n",
    "w_hat = w_b_hat[:-1]\n",
    "b_hat = w_b_hat[-1]\n",
    "\n",
    "print(\"True weights:\", w_true)\n",
    "print(\"True bias:\", b)\n",
    "print(\"Estimated weights (Least Squares):\", w_hat)\n",
    "print(\"Estimated bias (Least Squares):\", b_hat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4327ece-e228-4efb-ba03-75c84bade4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        nn.Module.__init__(self)\n",
    "        self.linear = nn.Linear(input_dim, 1)  # One output for regression\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8246d63-491e-4825-a87f-b6b2ac23882e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "\n",
    "X = ag.Tensor(x)\n",
    "yten = ag.expand_dims(ag.Tensor(y),1)\n",
    "model = LinearModel(3)\n",
    "\n",
    "num_epochs = 100\n",
    "start_time = time.time()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "losses_SGD = []\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X)\n",
    "    \n",
    "    loss = criterion(outputs, yten)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    losses_SGD.append(loss.value)\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.value:.10f}')\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "\n",
    "print(f\"Total training time: {total_time:.4f} seconds\")\n",
    "print(\"weight\", model.linear.weight.value)\n",
    "print(\"bias\",model.linear.bias.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccab23f7-6b39-4afb-8694-c4c1f53f52cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "\n",
    "X = ag.Tensor(x)\n",
    "yten = ag.expand_dims(ag.Tensor(y),1)\n",
    "model = LinearModel(3)\n",
    "\n",
    "num_epochs = 100\n",
    "start_time = time.time()\n",
    "\n",
    "optimizer = optim.SGDWithMomentum(model.parameters(), lr=0.01)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "losses_SGDwM = []\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X)\n",
    "    \n",
    "    loss = criterion(outputs, yten)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    losses_SGDwM.append(loss.value)\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.value:.10f}')\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "\n",
    "print(f\"Total training time: {total_time:.4f} seconds\")\n",
    "print(\"weight\", model.linear.weight.value)\n",
    "print(\"bias\",model.linear.bias.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58287e16-4839-4e50-9c85-f0161c8a6ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(losses_SGD, label='vanilla')\n",
    "plt.plot(losses_SGDwM, label='momentum')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c488d1f5-e5b3-490d-9db9-74a5f4309417",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
