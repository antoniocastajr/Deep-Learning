{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a83309-6e2e-4c8b-b03c-5eb59cd6d877",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743764bd-aa51-453b-99ff-5701c198364f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load numpy data\n",
    "challenge_x_train_np = np.load('challenge_x_train.npy')/255\n",
    "challenge_x_train_np = (challenge_x_train_np - 0.5) / 0.5  # Equivalent to transforms.Normalize((0.5,), (0.5,))\n",
    "\n",
    "challenge_y1_train_np = np.load('challenge_y1_train.npy')\n",
    "challenge_y2_train_np = np.load('challenge_y2_train.npy')\n",
    "plt.imshow(challenge_x_train_np[1],cmap = 'gray')\n",
    "plt.title(f\"Truth: {challenge_y1_train_np[1]}{challenge_y2_train_np[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb98170-a6a9-4137-9902-59949ed3a392",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Convert to torch tensors\n",
    "challenge_x_train = torch.tensor(challenge_x_train_np, dtype=torch.float32).unsqueeze(1)  # Add channel dimension (1 for grayscale)\n",
    "\n",
    "\n",
    "challenge_y1_train = torch.tensor(challenge_y1_train_np, dtype=torch.long)\n",
    "challenge_y2_train = torch.tensor(challenge_y2_train_np, dtype=torch.long)\n",
    "\n",
    "# Create dataset and split into training and validation\n",
    "dataset = TensorDataset(challenge_x_train, challenge_y1_train, challenge_y2_train)\n",
    "train_size = int(0.75 * len(dataset))\n",
    "valid_size = len(dataset) - train_size\n",
    "challenge_train, challenge_valid = random_split(dataset, [train_size, valid_size])\n",
    "\n",
    "challenge_train_loader = DataLoader(challenge_train, batch_size=64, shuffle=True)\n",
    "challenge_valid_loader = DataLoader(challenge_valid, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define LeNet-5 model\n",
    "class LeNet5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc_y1 = nn.Linear(84, 10)  # Output for y1\n",
    "        self.fc_y2 = nn.Linear(84, 10)  # Output for y2\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        y1 = self.fc_y1(x)\n",
    "        y2 = self.fc_y2(x)\n",
    "        return y1, y2\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "model = LeNet5()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training function\n",
    "def train_epoch(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    both_correct = 0\n",
    "    for x_batch, y1_batch, y2_batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        logits1, logits2 = model(x_batch)\n",
    "        loss_y1 = criterion(logits1, y1_batch)\n",
    "        loss_y2 = criterion(logits2, y2_batch)\n",
    "        loss = loss_y1 + loss_y2\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        _, y1_pred = torch.max(logits1, 1)\n",
    "        _, y2_pred = torch.max(logits2, 1)\n",
    "        both_correct += torch.sum((y1_pred == y1_batch) & (y2_pred == y2_batch)).item()\n",
    "    return total_loss / len(dataloader), both_correct / len(dataloader.dataset)\n",
    "\n",
    "# Validation function\n",
    "def validate_epoch(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    both_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y1_batch, y2_batch in dataloader:\n",
    "            logits1, logits2 = model(x_batch)\n",
    "            loss_y1 = criterion(logits1, y1_batch)\n",
    "            loss_y2 = criterion(logits2, y2_batch)\n",
    "            loss = loss_y1 + loss_y2\n",
    "            total_loss += loss.item()\n",
    "            _, y1_pred = torch.max(logits1, 1)\n",
    "            _, y2_pred = torch.max(logits2, 1)\n",
    "            both_correct += torch.sum((y1_pred == y1_batch) & (y2_pred == y2_batch)).item()\n",
    "    return total_loss / len(dataloader), both_correct / len(dataloader.dataset)\n",
    "\n",
    "# Training loop\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    train_loss, train_acc = train_epoch(model, challenge_train_loader, optimizer, criterion)\n",
    "    val_loss, val_acc = validate_epoch(model, challenge_valid_loader, criterion)\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Ep: {epoch + 1}/{epochs}, Tr Loss: {train_loss:.4f}, Tr Acc: {train_acc:.4f}, Va Loss: {val_loss:.4f}, Va Acc: {val_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2d3ee9-6ef3-441d-ba41-3cedd646b747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "challenge_x_test_tiny_np = np.load('challenge_x_test_tiny.npy')/255\n",
    "challenge_x_test_tiny_np = (challenge_x_test_tiny_np - 0.5) / 0.5  # Equivalent to transforms.Normalize((0.5,), (0.5,))\n",
    "challenge_y1_test_tiny_np = np.load('challenge_y1_test_tiny.npy')\n",
    "challenge_y2_test_tiny_np = np.load('challenge_y2_test_tiny.npy')\n",
    "\n",
    "# Convert to torch tensors\n",
    "challenge_x_test_tiny = torch.tensor(challenge_x_test_tiny_np, dtype=torch.float32).unsqueeze(1)  # Add channel dimension\n",
    "challenge_y1_test_tiny = torch.tensor(challenge_y1_test_tiny_np, dtype=torch.long)\n",
    "challenge_y2_test_tiny = torch.tensor(challenge_y2_test_tiny_np, dtype=torch.long)\n",
    "\n",
    "# Create DataLoader for test set\n",
    "challenge_test_tiny = TensorDataset(challenge_x_test_tiny, challenge_y1_test_tiny, challenge_y2_test_tiny)\n",
    "challenge_test_tiny_loader = DataLoader(challenge_test_tiny, batch_size=64, shuffle=False)\n",
    "\n",
    "# Test function (same as validation function)\n",
    "def test_model(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    both_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y1_batch, y2_batch in dataloader:\n",
    "            logits1, logits2 = model(x_batch)\n",
    "            loss_y1 = criterion(logits1, y1_batch)\n",
    "            loss_y2 = criterion(logits2, y2_batch)\n",
    "            loss = loss_y1 + loss_y2\n",
    "            total_loss += loss.item()\n",
    "            _, y1_pred = torch.max(logits1, 1)\n",
    "            _, y2_pred = torch.max(logits2, 1)\n",
    "            both_correct += torch.sum((y1_pred == y1_batch) & (y2_pred == y2_batch)).item()\n",
    "    return total_loss / len(dataloader), both_correct / len(dataloader.dataset)\n",
    "\n",
    "# After training, evaluate on test set\n",
    "test_loss, test_acc = test_model(model, challenge_test_tiny_loader, criterion)\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052be6e0-8ea8-448c-b595-29fd4934cef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits1, logits2 = model(challenge_x_test_tiny.data)\n",
    "\n",
    "_, y1_pred = torch.max(logits1, 1)\n",
    "_, y2_pred = torch.max(logits2, 1)\n",
    "y1_pred = y1_pred.numpy()\n",
    "y2_pred = y2_pred.numpy()\n",
    "\n",
    "wrong = (y1_pred == challenge_y1_test_tiny_np) & (y2_pred == challenge_y2_test_tiny_np)\n",
    "wrong = wrong <0.5\n",
    "i = np.where(wrong)[0][10]\n",
    "\n",
    "\n",
    "\n",
    "plt.imshow(challenge_x_test_tiny_np[i], cmap='gray')\n",
    "plt.title(f\"truth: {challenge_y1_test_tiny_np[i]}{challenge_y2_test_tiny_np[i]}, pred: {y1_pred[i]}{y2_pred[i]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe2b567-13a9-4393-b785-c3e040c8c0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the original MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "mnist_train = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "mnist_train_loader = DataLoader(mnist_train, batch_size=64, shuffle=True)\n",
    "\n",
    "mnist_test = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "mnist_test_loader = DataLoader(mnist_test, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define LeNet5 for MNIST\n",
    "class LeNet5_MNIST(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet5_MNIST, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)  # One output layer for 10 digits\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "model_mnist = LeNet5_MNIST()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_mnist.parameters(), lr=0.001)\n",
    "\n",
    "# File path for saving and loading model\n",
    "model_path = './lenet5_mnist.pth'\n",
    "\n",
    "# Check if model already exists, and load it if so\n",
    "if os.path.exists(model_path):\n",
    "    print(f'Loading saved model from {model_path}')\n",
    "    model_mnist.load_state_dict(torch.load(model_path, weights_only=True))\n",
    "else:\n",
    "    print('No saved model found, training from scratch.')\n",
    "\n",
    "    # Training loop for MNIST\n",
    "    epochs = 10\n",
    "    for epoch in range(epochs):\n",
    "        model_mnist.train()\n",
    "        total_loss = 0\n",
    "        for x_batch, y_batch in mnist_train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model_mnist(x_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f'Epoch {epoch+1}/{epochs}, MNIST Train Loss: {total_loss / len(mnist_train_loader):.4f}')\n",
    "\n",
    "    # Save the model after training\n",
    "    torch.save(model_mnist.state_dict(), model_path)\n",
    "    print(f'Model saved to {model_path}')\n",
    "\n",
    "# Evaluate on MNIST test set\n",
    "model_mnist.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_batch in mnist_test_loader:\n",
    "        y_pred = model_mnist(x_batch)\n",
    "        _, predicted = torch.max(y_pred, 1)\n",
    "        total += y_batch.size(0)\n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "mnist_acc = correct / total\n",
    "print(f'MNIST Test Accuracy: {mnist_acc:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000e1071-59f7-466c-8c4b-5c273e2e6809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the LeNet5 model by copying the weights from a pretrained model\n",
    "class LeNet5_Custom(nn.Module):\n",
    "    def __init__(self, pretrained_model):\n",
    "        super(LeNet5_Custom, self).__init__()\n",
    "        # Clone the layers from the pre-trained model\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)\n",
    "        self.conv1.weight = nn.Parameter(pretrained_model.conv1.weight.clone())\n",
    "        self.conv1.bias = nn.Parameter(pretrained_model.conv1.bias.clone())\n",
    "\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
    "        self.conv2.weight = nn.Parameter(pretrained_model.conv2.weight.clone())\n",
    "        self.conv2.bias = nn.Parameter(pretrained_model.conv2.bias.clone())\n",
    "\n",
    "        # Clone fully connected layers\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
    "        self.fc1.weight = nn.Parameter(pretrained_model.fc1.weight.clone())\n",
    "        self.fc1.bias = nn.Parameter(pretrained_model.fc1.bias.clone())\n",
    "\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc2.weight = nn.Parameter(pretrained_model.fc2.weight.clone())\n",
    "        self.fc2.bias = nn.Parameter(pretrained_model.fc2.bias.clone())\n",
    "\n",
    "        # New output layers for y1 and y2, cloned from fc3\n",
    "        self.fc_y1 = nn.Linear(84, 10)\n",
    "        self.fc_y1.weight = nn.Parameter(pretrained_model.fc3.weight.clone())\n",
    "        self.fc_y1.bias = nn.Parameter(pretrained_model.fc3.bias.clone())\n",
    "\n",
    "        self.fc_y2 = nn.Linear(84, 10)\n",
    "        self.fc_y2.weight = nn.Parameter(pretrained_model.fc3.weight.clone())\n",
    "        self.fc_y2.bias = nn.Parameter(pretrained_model.fc3.bias.clone())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        y1 = self.fc_y1(x)\n",
    "        y2 = self.fc_y2(x)\n",
    "        return y1, y2\n",
    "\n",
    "\n",
    "# File path for saving and loading model\n",
    "model_custom_path = './custom_lenet5.pth'\n",
    "\n",
    "# Initialize model with pre-trained layers\n",
    "model_custom = LeNet5_Custom(model_mnist)\n",
    "optimizer = optim.Adam(model_custom.parameters(), lr=0.001)\n",
    "\n",
    "# Check if the model already exists and load it if it does\n",
    "if os.path.exists(model_custom_path):\n",
    "    print(f'Loading saved fine-tuned model from {model_custom_path}')\n",
    "    model_custom.load_state_dict(torch.load(model_custom_path, weights_only=True))\n",
    "else:\n",
    "    print('No saved fine-tuned model found, training from scratch.')\n",
    "\n",
    "    # Training loop\n",
    "    epochs = 100\n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_acc = train_epoch(model_custom, challenge_train_loader, optimizer, criterion)\n",
    "        val_loss, val_acc = validate_epoch(model_custom, challenge_valid_loader, criterion)\n",
    "        print(f'Ep: {epoch + 1}/{epochs}, Tr Loss: {train_loss:.4f}, Tr Acc: {train_acc:.4f}, Va Loss: {val_loss:.4f}, Va Acc: {val_acc:.4f}')\n",
    "\n",
    "    # Save the fine-tuned model after training\n",
    "    torch.save(model_custom.state_dict(), model_custom_path)\n",
    "    print(f'Fine-tuned model saved to {model_custom_path}')\n",
    "    \n",
    "test_loss, test_acc = test_model(model_custom, challenge_test_tiny_loader, criterion)\n",
    "print(f'Test Loss: {test_loss:.4f}, Challenge Test Acc: {test_acc:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b49f74-bf0e-40b4-90c7-307b2c2c68b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits1, logits2 = model_custom(challenge_x_test_tiny.data)\n",
    "\n",
    "_, y1_pred = torch.max(logits1, 1)\n",
    "_, y2_pred = torch.max(logits2, 1)\n",
    "y1_pred = y1_pred.numpy()\n",
    "y2_pred = y2_pred.numpy()\n",
    "\n",
    "wrong = (y1_pred == challenge_y1_test_tiny_np) & (y2_pred == challenge_y2_test_tiny_np)\n",
    "wrong = wrong <0.5\n",
    "i = np.where(wrong)[0][8]\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(challenge_x_test_tiny_np[i],cmap='gray')\n",
    "plt.title(f\"truth: {challenge_y1_test_tiny_np[i]}{challenge_y2_test_tiny_np[i]}, pred: {y1_pred[i]}{y2_pred[i]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac05442-36d5-418c-9baf-b28c68050673",
   "metadata": {},
   "source": [
    "# Final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b2f1a3-86a3-4bfa-b2a2-9346e3304d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge_x_test_np = np.load('challenge_x_test.npy')/255\n",
    "challenge_x_test_np = (challenge_x_test_tiny_np - 0.5) / 0.5  # Equivalent to transforms.Normalize((0.5,), (0.5,))\n",
    "challenge_y1_test_np = np.load('challenge_y1_test.npy')\n",
    "challenge_y2_test_np = np.load('challenge_y2_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21927adb-7250-41d6-8dd6-2971718aebf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits1, logits2 = model_final(challenge_x_test.data)\n",
    "\n",
    "_, y1_pred = torch.max(logits1, 1)\n",
    "_, y2_pred = torch.max(logits2, 1)\n",
    "y1_pred = y1_pred.numpy()\n",
    "y2_pred = y2_pred.numpy()\n",
    "\n",
    "wrong = (y1_pred == challenge_y1_test_np) & (y2_pred == challenge_y2_test_np)\n",
    "wrong = wrong <0.5\n",
    "i = np.where(wrong)[0][8]\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(challenge_x_test_np[i],cmap='gray')\n",
    "plt.title(f\"truth: {challenge_y1_test_np[i]}{challenge_y2_test_np[i]}, pred: {y1_pred[i]}{y2_pred[i]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c280f5b7-550c-4dcf-b7d3-0258515e397e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
