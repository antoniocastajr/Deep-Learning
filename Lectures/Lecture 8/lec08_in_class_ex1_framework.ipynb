{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nsP6CD1CAK32"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "class ag: # AutoGrad\n",
        "    def log(input):\n",
        "        output = ag.Scalar(math.log(input.value), inputs=[input], op=\"log\")\n",
        "\n",
        "        def _backward():\n",
        "            input.grad += output.grad / input.value\n",
        "            return None\n",
        "\n",
        "        output._backward = _backward\n",
        "        return output\n",
        "\n",
        "    def exp(input):\n",
        "\n",
        "        output = ag.Scalar(math.exp(input.value), inputs=[input], op=\"exp\")\n",
        "\n",
        "        def _backward():\n",
        "            input.grad += output.grad * output.value\n",
        "            return None\n",
        "\n",
        "        output._backward = _backward\n",
        "        return output\n",
        "\n",
        "    def relu(input):\n",
        "        output = ag.Scalar(max(0, input.value), inputs=[input], op=\"relu\")\n",
        "\n",
        "        def _backward():\n",
        "            if input.value > 0:\n",
        "                input.grad += output.grad\n",
        "\n",
        "        output._backward = _backward\n",
        "        return output\n",
        "\n",
        "\n",
        "    class Scalar: # Scalars with grads\n",
        "        def __init__(self,  value, op=\"\", _backward= lambda : None, inputs=[], label=\"\"):\n",
        "\n",
        "            self.value = float(value)\n",
        "            self.grad = 0.0\n",
        "\n",
        "            self._backward = _backward\n",
        "            self.inputs = inputs\n",
        "\n",
        "            self.op = op\n",
        "            self.label = label\n",
        "\n",
        "\n",
        "        def topological_sort(self):\n",
        "            topo_order = []\n",
        "            visited = set()\n",
        "\n",
        "            def dfs(node):\n",
        "                if node not in visited:\n",
        "                    visited.add(node)\n",
        "                    for input in node.inputs:\n",
        "                        dfs(input)\n",
        "                    topo_order.append(node)\n",
        "\n",
        "            dfs(self)\n",
        "            return topo_order\n",
        "\n",
        "        def backward(self):\n",
        "            self.grad = 1.0\n",
        "\n",
        "            topo_order = self.topological_sort()\n",
        "\n",
        "            for node in reversed(topo_order):\n",
        "                node._backward()\n",
        "\n",
        "\n",
        "        def __add__(self, other):\n",
        "            assert isinstance(other, ag.Scalar)\n",
        "\n",
        "            output = ag.Scalar(self.value + other.value,\n",
        "                               inputs=[self, other], op=\"add\")\n",
        "\n",
        "            def _backward():\n",
        "                # pass\n",
        "                self.grad += output.grad\n",
        "                other.grad += output.grad\n",
        "\n",
        "            output._backward = _backward\n",
        "            return output\n",
        "\n",
        "\n",
        "        def __mul__(self, other):\n",
        "            assert isinstance(other, ag.Scalar)\n",
        "            output = ag.Scalar(self.value * other.value, inputs=[self, other], op=\"mul\")\n",
        "\n",
        "            def _backward():\n",
        "                self.grad += other.value * output.grad\n",
        "                other.grad += self.value * output.grad\n",
        "\n",
        "                return None\n",
        "\n",
        "            output._backward = _backward\n",
        "\n",
        "            return output\n",
        "\n",
        "        def __pow__(self, exponent): # exponent is just a python float\n",
        "            output = ag.Scalar(self.value ** exponent, inputs=[self], op=f\"pow({exponent})\")\n",
        "\n",
        "            def _backward():\n",
        "                self.grad += (exponent * self.value**(exponent-1)) * output.grad\n",
        "                return None\n",
        "\n",
        "            output._backward = _backward\n",
        "            return output\n",
        "\n",
        "        def __repr__(self) -> str:\n",
        "            return str(\"val:\" + str(self.value) + \", grad:\" + str(self.grad))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 1: fill in the the `YOUR CODE HERE` below"
      ],
      "metadata": {
        "id": "Zazvq_p11_Vv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Model:\n",
        "    def __init__(self, n_hidden,rng_seed = 42):\n",
        "        np.random.seed(rng_seed)\n",
        "\n",
        "        w1np = np.random.randn(n_hidden)\n",
        "        b1np = np.random.randn(n_hidden)\n",
        "        w2np = np.random.randn(n_hidden)\n",
        "        b2np = np.random.randn(1)\n",
        "\n",
        "        self.w1 = [ag.Scalar(val) for val in w1np]\n",
        "        self.b1 = [ag.Scalar(val) for val in b1np]\n",
        "        self.w2 = [ag.Scalar(val) for val in w2np]\n",
        "        self.b2 = [ag.Scalar(val) for val in b2np]\n",
        "\n",
        "        self.n_hidden = n_hidden\n",
        "\n",
        "        self.parameters = self.w1 + self.b1 + self.w2 + self.b2\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x is a 1-dimensional numpy array\n",
        "        # \"upgrade\" x into ag.Scalars\n",
        "        x_scalar = [ag.Scalar(val) for val in x]\n",
        "        n_samples = len(x_scalar)\n",
        "\n",
        "        ## YOUR CODE HERE\n",
        "        return [ag.Scalar(0.0) for i in range(n_samples)]\n",
        "\n",
        "\n",
        "\n",
        "class Loss:\n",
        "    def mse(self, predictions, targets):\n",
        "        # mean squared error\n",
        "        assert len(predictions) == len(targets)\n",
        "        n_samples = len(predictions)\n",
        "        loss = ag.Scalar(0.0)\n",
        "\n",
        "        ## YOUR CODE HERE\n",
        "        # make sure that you divide by total number of samples\n",
        "\n",
        "        return loss\n",
        "\n",
        "class Optimizer:\n",
        "    def __init__(self, parameters, lr=0.01):\n",
        "        self.parameters = parameters\n",
        "        self.lr = lr\n",
        "\n",
        "    def zero_grad(self):\n",
        "        for param in self.parameters:\n",
        "            param.grad = 0.0\n",
        "\n",
        "    def step(self):\n",
        "        for param in self.parameters:\n",
        "            param.value -= self.lr * param.grad\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zgqUCCz7s4BS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_numpy(ag_Scalar_list):\n",
        "    return np.array([scalar.value for scalar in ag_Scalar_list])\n"
      ],
      "metadata": {
        "id": "Zlvmvho5y1HY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate some synthetic data"
      ],
      "metadata": {
        "id": "xPyH179h6sBq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "N = 50\n",
        "\n",
        "np.random.seed(42)\n",
        "xnp = 2.0*(np.random.rand(N)-0.5) # note that i'm using rand instead of randn\n",
        "\n",
        "eps = (1/5)*np.random.randn(N)\n",
        "ynp = np.cos((3/2)*np.pi*xnp) + eps\n",
        "\n",
        "plt.scatter(xnp, ynp)"
      ],
      "metadata": {
        "id": "ZNYB6ECqs11w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualize the model at the beginning\n",
        "\n",
        "Note: the model will look like a flat line until you repair the `forward`"
      ],
      "metadata": {
        "id": "buuI26RA6t-D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_hidden = 20\n",
        "loss_fn = Loss()\n",
        "model = Model(n_hidden=n_hidden)\n",
        "output = model.forward(xnp)\n",
        "loss = loss_fn.mse(output, ynp)\n",
        "loss.backward()\n",
        "\n",
        "plt.scatter(xnp,ynp, label='train')\n",
        "plt.ylim([-1.5,2.0])\n",
        "\n",
        "x_grid = np.linspace(-1,1,100)\n",
        "\n",
        "y_grid = model.forward(x_grid)\n",
        "\n",
        "plt.plot(x_grid,to_numpy(y_grid))\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "oPaxmqEm6MFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check gradients!\n",
        "The following is from `lec03-in-class-ex2-relu-net`."
      ],
      "metadata": {
        "id": "9lkcZpPyouOF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def relu(z):\n",
        "    return np.maximum(0, z)\n",
        "\n",
        "def relu_derivative(z):\n",
        "    return 1.0*(z > 0)\n",
        "\n",
        "def forward_pass(x, theta):\n",
        "    w1, b1, w2, b2 = theta[\"w1\"], theta[\"b1\"], theta[\"w2\"], theta[\"b2\"]\n",
        "    h = np.dot(x, w1[np.newaxis,:]) + b1\n",
        "\n",
        "    z = np.dot(relu(h), w2) + b2\n",
        "\n",
        "    return h,z\n",
        "\n",
        "def compute_gradients(x, y, theta):\n",
        "    w1, b1, w2, b2 = theta[\"w1\"], theta[\"b1\"], theta[\"w2\"], theta[\"b2\"]\n",
        "    N = x.shape[0]\n",
        "\n",
        "    h, z = forward_pass(x, theta)\n",
        "\n",
        "    dJdw2 = -(2/N) *np.dot(y-z,relu(h))\n",
        "    dJdb2 = -(2/N) *np.sum(y-z)\n",
        "\n",
        "    dJdw1 = -(2/N)*np.dot(x.T,(y-z)[:,np.newaxis]*(relu_derivative(h)*w2)).flatten()\n",
        "    dJdb1 = -(2/N)*np.sum((y-z)[:,np.newaxis]*(relu_derivative(h)*w2),axis=0)\n",
        "\n",
        "    gradients = {\n",
        "        \"w1\": dJdw1,\n",
        "        \"b1\": dJdb1,\n",
        "        \"w2\": dJdw2,\n",
        "        \"b2\": dJdb2\n",
        "    }\n",
        "\n",
        "    return gradients\n",
        "np.random.seed(42)\n",
        "\n",
        "w1np = np.random.randn(n_hidden)\n",
        "b1np = np.random.randn(n_hidden)\n",
        "w2np = np.random.randn(n_hidden)\n",
        "b2np = np.random.randn(1)\n",
        "theta = {\n",
        "    \"w1\": w1np,\n",
        "    \"b1\": b1np,\n",
        "    \"w2\": w2np,\n",
        "    \"b2\": b2np\n",
        "}\n",
        "\n",
        "print(\"Gradients w.r.t...\")\n",
        "compute_gradients(xnp[:,np.newaxis],ynp,theta)"
      ],
      "metadata": {
        "id": "jR-_s2knmJoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check your autograds against the above"
      ],
      "metadata": {
        "id": "IBr7YbPg1a70"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.array([w1_.grad for w1_ in model.w1]))\n",
        "print(np.array([b1_.grad for b1_ in model.b1]))\n",
        "print(np.array([w2_.grad for w2_ in model.w2]))\n",
        "print(np.array([b2_.grad for b2_ in model.b2]))\n"
      ],
      "metadata": {
        "id": "y4QS1yKhnagg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train!"
      ],
      "metadata": {
        "id": "OzsQGEas6wIK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "n_hidden = 20\n",
        "model = Model(n_hidden=n_hidden)\n",
        "optimizer = Optimizer(model.parameters, lr=0.1)\n",
        "\n",
        "\n",
        "for epoch in range(100):\n",
        "    optimizer.zero_grad()\n",
        "    output = model.forward(xnp)\n",
        "    loss = loss_fn.mse(output, ynp)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Iteration {epoch}, Loss: {loss.value}\")"
      ],
      "metadata": {
        "id": "SauzfT3H6oxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualize the model at the end of the training\n",
        "Note: the model will look like a flat line until you repair the `forward`"
      ],
      "metadata": {
        "id": "CVRwpnxo64SS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(xnp,ynp, label='train')\n",
        "plt.ylim([-1.5,2.0])\n",
        "\n",
        "x_grid = np.linspace(-1,1,100)\n",
        "\n",
        "y_grid = model.forward(x_grid)\n",
        "\n",
        "plt.plot(x_grid,to_numpy(y_grid))\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "ivXWaGF021e6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lb5Tplhl2W5F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}