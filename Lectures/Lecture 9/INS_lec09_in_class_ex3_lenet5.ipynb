{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e01ea6ef-c429-445d-93ea-34fda59f052d",
      "metadata": {
        "id": "e01ea6ef-c429-445d-93ea-34fda59f052d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.sparse import csr_matrix\n",
        "import time\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed5f7058-b44b-4e9b-a447-cae0bedccee5",
      "metadata": {
        "id": "ed5f7058-b44b-4e9b-a447-cae0bedccee5"
      },
      "source": [
        "# Paste your `im2col_matrix_sparse` from the previous exercise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae7df9c9-1b37-4221-b0ed-66d639808b57",
      "metadata": {
        "id": "ae7df9c9-1b37-4221-b0ed-66d639808b57"
      },
      "outputs": [],
      "source": [
        "# im2col\n",
        "def im2col_matrix_sparse(Xin, K, S=1):\n",
        "    N, Cin, Hin, Win = Xin.shape\n",
        "    CHW = Cin * Hin * Win\n",
        "    Hout = (Hin - K)//S + 1\n",
        "    Wout = (Win - K)//S + 1\n",
        "    P = Hout * Wout  # Total number of patches per image\n",
        "    patch_size = Cin * K * K # Size of each flattened patch\n",
        "\n",
        "    data = [1 for _ in range(P*patch_size)]\n",
        "    row_indices = []\n",
        "    col_indices = list(range(P*patch_size))\n",
        "\n",
        "    patch_idx = 0\n",
        "    for hout in range(Hout):\n",
        "        for wout in range(Wout):\n",
        "            for cin in range(Cin):\n",
        "                for hker in range(K):\n",
        "                    for wker in range(K):\n",
        "                        input_index = cin * Hin * Win + hout * S * Win + wout * S + hker * Win + wker\n",
        "                        row_indices.append(input_index)\n",
        "            patch_idx += 1\n",
        "\n",
        "    im2col_mat_sparse = csr_matrix((data, (row_indices, col_indices)), shape=(CHW, P * patch_size))\n",
        "    return im2col_mat_sparse\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # im2col for dense matrix\n",
        "# def im2col_matrix_dense(Xin, K, S=1):\n",
        "#     N, Cin, Hin, Win = Xin.shape\n",
        "#     Hout, Wout = (Hin - K)//S + 1, (Win - K)//S + 1\n",
        "#     P = Hout * Wout  # Total number of patches per image\n",
        "#     patch_size = Cin * K * K # Size of each flattened patch\n",
        "#     im2col_mat_dense = np.zeros((Cin * Hin * Win, P * patch_size))\n",
        "#     patch_idx = 0\n",
        "#     output_index = 0\n",
        "#     for hout in range(Hout):\n",
        "#         for wout in range(Wout):\n",
        "#             for cin in range(Cin):\n",
        "#                 for hker in range(K):\n",
        "#                     for wker in range(K):\n",
        "#                         input_index = cin * Hin * Win + hout * S * Win + wout * S + hker * Win + wker\n",
        "#                         im2col_mat_dense[input_index, output_index] = 1\n",
        "#                         output_index += 1\n",
        "#             patch_idx += 1\n",
        "#     return im2col_mat_dense\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65a9c98f-0dac-4fc4-b647-b2fe288763f4",
      "metadata": {
        "id": "65a9c98f-0dac-4fc4-b647-b2fe288763f4"
      },
      "outputs": [],
      "source": [
        "# broadcasting\n",
        "\n",
        "# Broadcasted shape\n",
        "def broadcasted_shape(shape_X, shape_Y):\n",
        "\n",
        "    max_len = max(len(shape_X), len(shape_Y))\n",
        "    min_len = min(len(shape_X), len(shape_Y))\n",
        "    swapped = False\n",
        "\n",
        "    if len(shape_X) < max_len:\n",
        "        swapped = True\n",
        "        L, S = shape_Y, shape_X # L = long, S = short\n",
        "    else:\n",
        "        L, S = shape_X, shape_Y\n",
        "\n",
        "    L_rev = L[::-1]\n",
        "    S_rev = S[::-1]\n",
        "\n",
        "    result_shape = []\n",
        "    axes_L_expanded = []\n",
        "    axes_S_expanded = []\n",
        "\n",
        "    for i in range(min_len):\n",
        "        dim_L = L_rev[i]\n",
        "        dim_S = S_rev[i]\n",
        "        if dim_L == 1 and dim_S != 1:\n",
        "            axes_L_expanded.append(max_len  -1- i)\n",
        "        elif dim_L != 1 and dim_S == 1:\n",
        "            axes_S_expanded.append(max_len  -1- i)\n",
        "        if dim_L == 1 or dim_S == 1 or dim_L == dim_S:\n",
        "            result_shape.append(max(dim_L,dim_S))\n",
        "        else:\n",
        "            raise ValueError(f\"Shapes {shape_X} and {shape_Y} not broadcastable\")\n",
        "\n",
        "\n",
        "    result_shape += L_rev[(min_len):]\n",
        "\n",
        "    result_shape = tuple(result_shape[::-1])\n",
        "    axes_L_expanded = tuple(axes_L_expanded[::-1])\n",
        "    axes_S_expanded = tuple(axes_S_expanded[::-1])\n",
        "\n",
        "    if swapped:\n",
        "        return result_shape, axes_S_expanded, max_len - min_len,  axes_L_expanded, 0\n",
        "    else:\n",
        "        return result_shape,  axes_L_expanded, 0, axes_S_expanded, max_len - min_len\n",
        "\n",
        "def unbroadcast(arr, ax, pad):\n",
        "    return np.sum(np.sum(arr, axis = ax, keepdims = True), axis = tuple(range(pad)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7cab3cba-7178-43b2-b9b8-ed180df277ee",
      "metadata": {
        "id": "7cab3cba-7178-43b2-b9b8-ed180df277ee"
      },
      "outputs": [],
      "source": [
        "class ag: # AutoGrad\n",
        "\n",
        "    #################\n",
        "    # ENTRYWISE OPS #\n",
        "    #################\n",
        "    def log(input):\n",
        "        output = ag.Tensor(np.log(input.value), inputs=[input], op=\"log\")\n",
        "        def _backward():\n",
        "            input.grad += output.grad / input.value\n",
        "            return None\n",
        "        output._backward = _backward\n",
        "        return output\n",
        "\n",
        "    def exp(input):\n",
        "\n",
        "        output = ag.Tensor(np.exp(input.value), inputs=[input], op=\"exp\")\n",
        "\n",
        "        def _backward():\n",
        "            input.grad += output.grad * output.value\n",
        "            return None\n",
        "\n",
        "        output._backward = _backward\n",
        "        return output\n",
        "\n",
        "    def relu(input):\n",
        "        output = ag.Tensor(np.maximum(0, input.value), inputs=[input], op=\"relu\")\n",
        "\n",
        "        def _backward():\n",
        "            input.grad += (input.value > 0)*output.grad\n",
        "            return None\n",
        "\n",
        "        output._backward = _backward\n",
        "        return output\n",
        "\n",
        "\n",
        "\n",
        "    #################\n",
        "    # REDUCTIVE OPS #\n",
        "    #################\n",
        "    def sum(input,axis = None, keepdims = False):\n",
        "        output = ag.Tensor(np.sum(input.value, axis = axis, keepdims = keepdims), inputs = [input], op='sum')\n",
        "        def _backward():\n",
        "            if axis == None or keepdims:\n",
        "                input.grad += output.grad\n",
        "            else:\n",
        "                input.grad += np.expand_dims(output.grad, axis = axis)\n",
        "            return None\n",
        "        output._backward = _backward\n",
        "        return output\n",
        "\n",
        "    def matmul(input1, input2):\n",
        "        return input1@input2\n",
        "    ###############\n",
        "    # SHAPING OPS #\n",
        "    ###############\n",
        "\n",
        "    def expand_dims(input, axis):\n",
        "        output = ag.Tensor(np.expand_dims(input.value,axis=axis), inputs = [input])\n",
        "        def _backward():\n",
        "            input.grad += np.squeeze(output.grad, axis = axis)\n",
        "            return None\n",
        "        output._backward = _backward\n",
        "        return output\n",
        "\n",
        "    def reshape(input, newshape):\n",
        "        output = ag.Tensor(np.reshape(input.value, newshape), inputs=[input], op=\"reshape\")\n",
        "        def _backward():\n",
        "            input.grad += np.reshape(output.grad, input.shape)\n",
        "            return None\n",
        "        output._backward = _backward\n",
        "        return output\n",
        "\n",
        "    def moveaxis(input, source, destination):\n",
        "        output = ag.Tensor(np.moveaxis(input.value, source, destination), inputs=[input], op=\"moveaxis\")\n",
        "\n",
        "        def _backward():\n",
        "            input.grad += np.moveaxis(output.grad, source, destination)\n",
        "            return None\n",
        "        output._backward = _backward\n",
        "        return output\n",
        "\n",
        "    # [sp]arse [c]onstant (non-AG-enabled) [mat]rix [mul]tiplication\n",
        "    def spcmatmul(input, sparse_mat: csr_matrix):\n",
        "        output = ag.Tensor(input.value @ sparse_mat,\n",
        "                           inputs = [input],\n",
        "                           op=\"spcmatmul\")\n",
        "\n",
        "        def _backward():\n",
        "            input.grad += output.grad @ sparse_mat.T\n",
        "            return None\n",
        "\n",
        "        output._backward = _backward\n",
        "        return output\n",
        "\n",
        "    class Tensor: # Tensor with grads\n",
        "        def __init__(self,\n",
        "                     value,\n",
        "                     op=\"\",\n",
        "                     _backward= lambda : None,\n",
        "                     inputs=[],\n",
        "                     label=\"\",\n",
        "                     requires_grad=True):\n",
        "\n",
        "            if type(value) in [float ,int]:\n",
        "                value = np.array(value).astype(np.float64)\n",
        "            self.value = value.astype(np.float64)\n",
        "            self.grad = np.zeros_like(self.value).astype(np.float64)\n",
        "\n",
        "            self.shape = value.shape\n",
        "\n",
        "            self._backward = _backward\n",
        "            self.inputs = inputs\n",
        "\n",
        "            self.op = op\n",
        "            self.label = label\n",
        "\n",
        "            self.requires_grad = requires_grad\n",
        "\n",
        "        def topological_sort(self):\n",
        "            topo_order = []\n",
        "            visited = set()\n",
        "\n",
        "            def dfs(node):\n",
        "                if node not in visited:\n",
        "                    visited.add(node)\n",
        "                    for input in node.inputs:\n",
        "                        dfs(input)\n",
        "                    topo_order.append(node)\n",
        "\n",
        "            dfs(self)\n",
        "            return topo_order\n",
        "\n",
        "        def backward(self):\n",
        "            self.grad = np.array(1.0).astype(np.float64)\n",
        "\n",
        "            topo_order = self.topological_sort()\n",
        "\n",
        "            for node in reversed(topo_order):\n",
        "                node._backward()\n",
        "\n",
        "\n",
        "\n",
        "        def __add__(self, other):\n",
        "\n",
        "            if type(other) in [float, int]:\n",
        "                other = ag.Tensor(1.0*other)\n",
        "            result_shape, ax1, pad1, ax2, pad2 = broadcasted_shape(self.shape, other.shape)\n",
        "\n",
        "            output = ag.Tensor(self.value + other.value,\n",
        "                               inputs=[self, other], op=\"add\")\n",
        "            def _backward():\n",
        "                self.grad += unbroadcast(output.grad, ax1, pad1)\n",
        "                other.grad += unbroadcast(output.grad, ax2, pad2)\n",
        "\n",
        "            output._backward = _backward\n",
        "            return output\n",
        "\n",
        "        def __sub__(self,other):\n",
        "            return self + other*(-1)\n",
        "\n",
        "        def __neg__(self):\n",
        "            output = ag.Tensor(-self.value, inputs=[self], op=\"neg\")\n",
        "            def _backward():\n",
        "                self.grad -= output.grad\n",
        "                return None\n",
        "            output._backward = _backward\n",
        "            return output\n",
        "\n",
        "        def __mul__(self, other):\n",
        "            if type(other) in [float, int]:\n",
        "                other = ag.Tensor(1.0*other)\n",
        "            result_shape, ax1, pad1, ax2, pad2 = broadcasted_shape(self.shape, other.shape)\n",
        "\n",
        "            output = ag.Tensor(self.value * other.value,\n",
        "                               inputs=[self, other], op=\"mul\")\n",
        "            def _backward():\n",
        "                self.grad += unbroadcast(output.grad*other.value, ax1, pad1)\n",
        "                other.grad += unbroadcast(output.grad*self.value, ax2, pad2)\n",
        "\n",
        "            output._backward = _backward\n",
        "            return output\n",
        "\n",
        "        def __truediv__(self,other):\n",
        "            return self*(other**(-1))\n",
        "\n",
        "\n",
        "        def __pow__(self, exponent): # exponent is just a python float\n",
        "            output = ag.Tensor(self.value ** exponent,\n",
        "                               inputs=[self],\n",
        "                               op=f\"pow({exponent})\")\n",
        "\n",
        "            def _backward():\n",
        "\n",
        "                self.grad += (exponent * self.value**(exponent-1)) * output.grad\n",
        "                return None\n",
        "\n",
        "            output._backward = _backward\n",
        "            return output\n",
        "\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            output = ag.Tensor(np.array(self.value[idx]),\n",
        "                               inputs = [self],\n",
        "                               op=f\"[...]\")\n",
        "            def _backward():\n",
        "                self.grad[idx] += output.grad # idx must not have repeats!\n",
        "                return None\n",
        "            output._backward = _backward\n",
        "            return output\n",
        "\n",
        "        def __radd__(self, other):\n",
        "            return self + other\n",
        "\n",
        "        def __rmul__(self, other):\n",
        "            return self * other\n",
        "\n",
        "        def __rsub__(self, other):\n",
        "            return (-self) + other\n",
        "\n",
        "        def __rtruediv__(self, other):\n",
        "            return ag.Tensor(other) / self\n",
        "\n",
        "\n",
        "        def __matmul__(self,other):\n",
        "            output = ag.Tensor(np.matmul(self.value,other.value),\n",
        "                               inputs = [self,other],\n",
        "                               op=\"matmul\")\n",
        "\n",
        "            # self.shape = (a,b,c, m,n)\n",
        "            # other.shape = (a,b,c, n,p)\n",
        "            # output.grad.shape == output.value.shape = (a,b,c, m,p)\n",
        "\n",
        "\n",
        "            def _backward():\n",
        "                if len(other.value.shape) == 1:\n",
        "                    if len(self.value.shape) == 1:\n",
        "                        raise Exception(\"To take the dot-product of two vectors, use ag.sum(x*y) instead.\")\n",
        "                    else:\n",
        "                        self.grad += np.matmul(output.grad[:,None], other.value[None,:])\n",
        "                else:\n",
        "                    self.grad += np.matmul(output.grad, np.moveaxis(other.value,-1,-2))\n",
        "                axis = list(range(len(self.value.shape)-1))\n",
        "                other.grad += np.tensordot(self.value, output.grad, axes = [axis,axis])\n",
        "                return None\n",
        "            output._backward = _backward\n",
        "            return output\n",
        "\n",
        "        def reshape(self, *newshape):\n",
        "            output = ag.Tensor(np.reshape(self.value, tuple(newshape)), inputs=[self], op=\"reshape\")\n",
        "            def _backward():\n",
        "                self.grad += np.reshape(output.grad, self.shape)\n",
        "                return None\n",
        "            output._backward = _backward\n",
        "            return output\n",
        "\n",
        "\n",
        "\n",
        "        def __repr__(self) -> str:\n",
        "            return \"Value:\\n\"+self.value.__repr__() + \"\\nGrad:\\n\" + self.grad.__repr__()\n",
        "        # def __repr__(self) -> str:\n",
        "        #     return \"Value:\\n\"+self.value.__repr__() + \"\\nGrad:\\n\" + self.grad.__repr__()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88f7514f-7970-4817-98a1-b605ce83d2c0",
      "metadata": {
        "id": "88f7514f-7970-4817-98a1-b605ce83d2c0"
      },
      "outputs": [],
      "source": [
        "class nn:\n",
        "    # A fake python class so that we can use nn as an \"namespace\"\n",
        "    # This way we can avoid having an actual python module\n",
        "    # which would create more files for you to have to download\n",
        "    # and for me to upload.\n",
        "\n",
        "    # But the sacrifice is that the class inheritance is a bit wonky\n",
        "    # Maybe someone more well-versed in python class inheritance can help me out\n",
        "    # Email me if you'd like to contribute to making this class better next time\n",
        "\n",
        "    class Module:\n",
        "        def __init__(self):\n",
        "            self._parameters = {}\n",
        "            self._modules = {}\n",
        "\n",
        "        def forward(self, *inputs):\n",
        "            raise NotImplementedError\n",
        "\n",
        "        def __call__(self, *inputs):\n",
        "            return self.forward(*inputs)\n",
        "\n",
        "        def parameters(self):\n",
        "            params = list(self._parameters.values())\n",
        "            for module in self._modules.values():\n",
        "                params.extend(module.parameters())\n",
        "            return params\n",
        "\n",
        "        def __setattr__(self, name, value):\n",
        "            object.__setattr__(self, name, value)\n",
        "            if isinstance(value, nn.Module):\n",
        "                self._modules[name] = value\n",
        "            elif isinstance(value, ag.Tensor):\n",
        "                self._parameters[name] = value\n",
        "\n",
        "\n",
        "    class Linear(Module):\n",
        "        def __init__(self, in_features, out_features):\n",
        "            nn.Module.__init__(self)\n",
        "            kaiming_he_init_constant = np.sqrt(2 / in_features)\n",
        "\n",
        "            self.weight = ag.Tensor(np.random.randn(in_features, out_features) * kaiming_he_init_constant )\n",
        "            self.bias = ag.Tensor(np.zeros(out_features))\n",
        "\n",
        "            # these do not trigger __setattr__\n",
        "            self._parameters['weight'] = self.weight\n",
        "            self._parameters['bias'] = self.bias\n",
        "\n",
        "        def forward(self, x):\n",
        "            return ag.matmul(x, self.weight) + self.bias\n",
        "\n",
        "    class AvgPool2d(Module):\n",
        "        def __init__(self, kernel_size):\n",
        "            super().__init__()\n",
        "            self.kernel_size = kernel_size\n",
        "            self.im2col_mat = None\n",
        "\n",
        "        def forward(self, Xin):\n",
        "            N, Cin, Hin, Win = Xin.shape\n",
        "            assert(Hin % self.kernel_size == 0)\n",
        "            assert(Win % self.kernel_size == 0)\n",
        "\n",
        "            K = self.kernel_size\n",
        "            S = self.kernel_size\n",
        "\n",
        "            # Yang Xu's more slick implementation\n",
        "            Hout = Hin//S\n",
        "            Wout = Win//S\n",
        "\n",
        "            Xin_patches = Xin.reshape(N, Cin, Hout, K, Wout, K)\n",
        "            Xout = ag.sum(Xin_patches, axis=(-1, -3))/K**2\n",
        "\n",
        "            # Hout = (Hin - K) // S + 1\n",
        "            # Wout = (Win - K) // S + 1\n",
        "\n",
        "            # P = Hout * Wout  # Total number of patches per image\n",
        "            # patch_size = Cin * K * K\n",
        "\n",
        "            # Xin_flat = Xin.reshape(-1, Cin * Hin * Win)\n",
        "\n",
        "            # if self.im2col_mat is None:\n",
        "            #     self.im2col_mat = im2col_matrix_sparse(Xin, K, S)\n",
        "\n",
        "            # Xin_im2col = ag.spcmatmul(Xin_flat, self.im2col_mat)\n",
        "            # Xin_patches_flat = Xin_im2col.reshape(N, P, patch_size)\n",
        "\n",
        "            # Xin_cw_patches_flat = Xin_patches_flat.reshape(N, P, Cin, K**2)\n",
        "            # Xout = ag.moveaxis(ag.sum(Xin_cw_patches_flat, axis=-1), -1, -2).reshape(N, Cin, Hout, Wout) / K**2\n",
        "\n",
        "            return Xout\n",
        "\n",
        "    class Conv2d(Module):\n",
        "        def __init__(self, in_channels, out_channels, kernel_size, stride=1):\n",
        "            super().__init__()\n",
        "            self.in_channels = in_channels # Cin\n",
        "            self.out_channels = out_channels # Cout\n",
        "            self.kernel_size = kernel_size # K\n",
        "            self.stride = stride # S\n",
        "\n",
        "            kaiming_he_init_constant = np.sqrt(2 / (in_channels * kernel_size**2))\n",
        "\n",
        "            # these are flattened weights\n",
        "            self.weight = ag.Tensor(np.random.randn(in_channels*kernel_size**2, out_channels) * kaiming_he_init_constant)\n",
        "\n",
        "            # bias are set to zeros initially\n",
        "            self.bias = ag.Tensor(np.zeros(out_channels))\n",
        "\n",
        "            self._parameters['weight'] = self.weight\n",
        "            self._parameters['bias'] = self.bias\n",
        "\n",
        "            self.im2col_mat = None # im2col_mat will be cached\n",
        "\n",
        "        def forward(self, Xin):\n",
        "            # Xin: (N, Cin, Hin, Win)\n",
        "            N, Cin, Hin, Win = Xin.shape\n",
        "            assert(Cin == self.in_channels)\n",
        "\n",
        "            K = self.kernel_size\n",
        "            S = self.stride\n",
        "            Hout = (Hin - K) // S + 1\n",
        "            Wout = (Win - K) // S + 1\n",
        "            P = Hout * Wout  # Total number of patches per image\n",
        "            patch_size = Cin * K * K  # Size of each flattened patch\n",
        "            Cout = self.out_channels\n",
        "\n",
        "            Xin_flat = Xin.reshape(-1, Cin * Hin * Win)\n",
        "\n",
        "            # Cache the im2col matrix\n",
        "            if self.im2col_mat is None:\n",
        "                self.im2col_mat = im2col_matrix_sparse(Xin, K, S)\n",
        "\n",
        "            Xin_im2col = ag.spcmatmul(Xin_flat, self.im2col_mat)\n",
        "            Xin_patches_flat = Xin_im2col.reshape(N, P, patch_size)\n",
        "\n",
        "\n",
        "            Xout_flat = (Xin_patches_flat @ self.weight) + self.bias\n",
        "            Xout_flat = ag.moveaxis(Xout_flat, 1, 2)\n",
        "            Xout = Xout_flat.reshape(N, Cout, Hout, Wout)\n",
        "\n",
        "            # Xout: (N, Cout, Hout, Wout)\n",
        "            return Xout\n",
        "\n",
        "\n",
        "    class MSELoss:\n",
        "        def __call__(self, input, target):\n",
        "            N = target.value.shape[0]\n",
        "            return ag.sum((target - input)**2) / N\n",
        "\n",
        "    class BinaryCrossEntropyLoss:\n",
        "        def __call__(self, input, target):\n",
        "            N = target.value.shape[0]\n",
        "            return ag.sum( ag.log(1.0+ ag.exp(-input*target))) / N\n",
        "\n",
        "    class CrossEntropyLoss:\n",
        "        def __call__(self, input, target):\n",
        "            # input should be an ag.Tensor of shape (N, num_classes)\n",
        "            # target should be a numpy array of shape (N,) of elements of [0,1,..., num_classes-1]\n",
        "\n",
        "            z = input\n",
        "            y = target\n",
        "\n",
        "            expz = ag.exp(z)\n",
        "\n",
        "            p = expz / ag.sum(expz, axis=-1, keepdims=True)\n",
        "            N = z.shape[0]\n",
        "            l = ag.sum(-ag.log(p[np.arange(N), y]))/N\n",
        "            return l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7503a2a3-c94d-4f05-a97a-92d17a16a9f4",
      "metadata": {
        "id": "7503a2a3-c94d-4f05-a97a-92d17a16a9f4"
      },
      "outputs": [],
      "source": [
        "class optim:\n",
        "    class SGD:\n",
        "        def __init__(self, parameters, lr=0.01):\n",
        "            self.parameters = parameters\n",
        "            self.lr = lr\n",
        "\n",
        "        def step(self):\n",
        "            for p in self.parameters:\n",
        "                p.value -= self.lr * p.grad\n",
        "\n",
        "        def zero_grad(self):\n",
        "            for p in self.parameters:\n",
        "                p.grad = np.zeros_like(p.value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c3a6064-5508-4575-b750-bf1dd3de3031",
      "metadata": {
        "id": "2c3a6064-5508-4575-b750-bf1dd3de3031"
      },
      "outputs": [],
      "source": [
        "loaded_images = (1.0/255)*np.loadtxt('lec09_images.csv', delimiter=\",\").reshape(-1,1,28,28)\n",
        "\n",
        "# padding\n",
        "loaded_images = np.pad(loaded_images, ((0,0), (0,0), (2,2), (2,2))) # ensure 32-by-32, as in LeCun et al 1998\n",
        "# this is a preprocess step, no need to track this op through out autograd\n",
        "# although later on, it might be a good idea to add \"pad\" as an op\n",
        "\n",
        "X = ag.Tensor(1.0*loaded_images)\n",
        "y = np.repeat(np.arange(10), 8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe97ea2e-7359-47ef-9e06-8587b7bb7fd7",
      "metadata": {
        "id": "fe97ea2e-7359-47ef-9e06-8587b7bb7fd7"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(2, 5, figsize=(6, 3))\n",
        "\n",
        "for i in range(10):\n",
        "    ax = axes[i // 5, i % 5]\n",
        "    ax.imshow(loaded_images[i * 8][0], cmap='gray')\n",
        "    ax.set_title(y[i*8])\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing your convolution layer"
      ],
      "metadata": {
        "id": "qBqQcSgjQuRD"
      },
      "id": "qBqQcSgjQuRD"
    },
    {
      "cell_type": "code",
      "source": [
        "conv_toy_example = nn.Conv2d(in_channels=1,out_channels=2, kernel_size=2)\n",
        "\n",
        "\n",
        "conv_toyexweight = np.array([[ 0.,  -1.],\n",
        "                             [ 1.,  0.],\n",
        "                             [-1.,  1.],\n",
        "                             [ 0.,  0.]])\n",
        "conv_toy_example.weight = ag.Tensor(conv_toyexweight)\n",
        "\n",
        "X_conved = conv_toy_example(X)"
      ],
      "metadata": {
        "id": "q-VV76_FP3Th"
      },
      "id": "q-VV76_FP3Th",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Channel 1"
      ],
      "metadata": {
        "id": "USfEb53TQXxa"
      },
      "id": "USfEb53TQXxa"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "fig, axes = plt.subplots(2, 5, figsize=(6, 3))\n",
        "\n",
        "for i in range(10):\n",
        "    ax = axes[i // 5, i % 5]\n",
        "    ax.imshow(X_conved.value[i * 8][0], cmap='gray')\n",
        "    ax.set_title(y[i*8])\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "c1YQ6n_3QWFn"
      },
      "id": "c1YQ6n_3QWFn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Channel 2"
      ],
      "metadata": {
        "id": "XNhAYZUjQbUP"
      },
      "id": "XNhAYZUjQbUP"
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(2, 5, figsize=(6, 3))\n",
        "\n",
        "for i in range(10):\n",
        "    ax = axes[i // 5, i % 5]\n",
        "    ax.imshow(X_conved.value[i * 8][1], cmap='gray')\n",
        "    ax.set_title(y[i*8])\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "89fkhwhuQVLq"
      },
      "id": "89fkhwhuQVLq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing your AvgPool layer"
      ],
      "metadata": {
        "id": "TDOvLREcQ1Go"
      },
      "id": "TDOvLREcQ1Go"
    },
    {
      "cell_type": "code",
      "source": [
        "avgpool_test = nn.AvgPool2d(kernel_size=2)\n",
        "\n",
        "\n",
        "X_pooled = avgpool_test(X)\n",
        "\n",
        "fig, axes = plt.subplots(2, 5, figsize=(6, 3))\n",
        "\n",
        "for i in range(10):\n",
        "    ax = axes[i // 5, i % 5]\n",
        "    ax.imshow(X_pooled.value[i * 8][0], cmap='gray')\n",
        "    ax.set_title(y[i*8])\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IL-JS_Z_QyrA"
      },
      "id": "IL-JS_Z_QyrA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f21dbde-ad90-45e8-bb65-454c8a793759",
      "metadata": {
        "id": "2f21dbde-ad90-45e8-bb65-454c8a793759"
      },
      "outputs": [],
      "source": [
        "class LeNet5(nn.Module):\n",
        "    def __init__(self):\n",
        "        nn.Module.__init__(self)\n",
        "\n",
        "        # Convolutional layers (Conv2d + AvgPool2d)\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
        "        self.avgpool1 = nn.AvgPool2d(2)\n",
        "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5)\n",
        "        self.avgpool2 = nn.AvgPool2d(2)\n",
        "\n",
        "        # Fully-connected layers\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Convolutional (conv) component of the network\n",
        "        x = self.conv1(x)\n",
        "        x = ag.relu(x)\n",
        "        x = self.avgpool1(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = ag.relu(x)\n",
        "        x = self.avgpool2(x)\n",
        "\n",
        "        # Flatten the output from conv layers to feed into fully connected layers\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "\n",
        "        # Fully-connected (fc) component of the network\n",
        "        x = self.fc1(x)\n",
        "        x = ag.relu(x)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        x = ag.relu(x)\n",
        "\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4b73ac1-253d-4875-b3fd-c69d232e78fc",
      "metadata": {
        "id": "f4b73ac1-253d-4875-b3fd-c69d232e78fc"
      },
      "outputs": [],
      "source": [
        "model = LeNet5()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How good is the model initially?"
      ],
      "metadata": {
        "id": "0Be3HyUNID5p"
      },
      "id": "0Be3HyUNID5p"
    },
    {
      "cell_type": "code",
      "source": [
        "np.argmax(model(X).value,axis=-1) == y"
      ],
      "metadata": {
        "id": "zU6Bl3n0H2Y2"
      },
      "id": "zU6Bl3n0H2Y2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Count the number of parameters"
      ],
      "metadata": {
        "id": "y7vLrI4HH3KU"
      },
      "id": "y7vLrI4HH3KU"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63104228-9503-4134-88c2-86221327c0e9",
      "metadata": {
        "id": "63104228-9503-4134-88c2-86221327c0e9"
      },
      "outputs": [],
      "source": [
        "np.sum([np.prod(param.value.shape) for param in list(model.parameters())])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dca2c0e6-ba07-4ae3-9b8c-326c521827a1",
      "metadata": {
        "id": "dca2c0e6-ba07-4ae3-9b8c-326c521827a1"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "num_epochs = 100\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(X)\n",
        "    loss = criterion(outputs, y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.value:.10f}')\n",
        "end_time = time.time()\n",
        "total_time = end_time - start_time\n",
        "\n",
        "print(f\"Total training time: {total_time:.4f} seconds\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# It should achieve 100 percent training acc"
      ],
      "metadata": {
        "id": "VGwLA5aoSWMG"
      },
      "id": "VGwLA5aoSWMG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95514fe3-67d8-46ab-a841-262fdc83e15a",
      "metadata": {
        "id": "95514fe3-67d8-46ab-a841-262fdc83e15a"
      },
      "outputs": [],
      "source": [
        "np.argmax(model(X).value,axis=-1) == y"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}