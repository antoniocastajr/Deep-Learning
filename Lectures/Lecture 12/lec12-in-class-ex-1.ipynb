{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1eed5d57-839e-4105-b40b-bebff1c5ed99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "matmul_call_count = 0 # A counter to keep track of matmul calls\n",
    "\n",
    "def reset_matmul_call_counter():\n",
    "    global matmul_call_count\n",
    "    matmul_call_count = 0\n",
    "\n",
    "def matmul_counter_wrapper(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        global matmul_call_count\n",
    "        matmul_call_count += 1\n",
    "        return func(*args, **kwargs)\n",
    "    return wrapper\n",
    "\n",
    "# Replace np.matmul with the logged version\n",
    "np.matmul = matmul_counter_wrapper(np.matmul)\n",
    "\n",
    "class ag: # AutoGrad\n",
    "    \"\"\"\n",
    "    A barebone version of the AutoGrad library that we've been working with\n",
    "    it only supports \n",
    "    - matmul \n",
    "        - between two numpy matrices ONLY,\n",
    "            -i.e., arrays X such that len(X.shape) == 2\n",
    "    - sum \n",
    "        - \"axis = None\" ONLY\n",
    "            - so everything gets summed up\n",
    "    - add (entrywise)\n",
    "    \"\"\"\n",
    "\n",
    "    #################\n",
    "    # REDUCTIVE OPS #\n",
    "    #################\n",
    "    def sum(input):\n",
    "        output = ag.Tensor(np.sum(input.value), inputs = [input], op='sum')\n",
    "        def _backward():\n",
    "            # YOUR CODE HERE FOR initialize the grad if they are none\n",
    "            input.grad += output.grad\n",
    "            # YOUR CODE HERE FOR discarding the grad\n",
    "            return None\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "    ##########\n",
    "    # MATMUL #\n",
    "    ##########\n",
    "    def matmul(input1, input2):\n",
    "        return input1@input2\n",
    "\n",
    "    class Tensor: # Tensor with grads\n",
    "        def __init__(self,\n",
    "                     value,\n",
    "                     requires_grad=False,\n",
    "                     op=\"\",\n",
    "                     _backward= lambda : None,\n",
    "                     inputs=[],\n",
    "                     label=\"\"):\n",
    "\n",
    "            if type(value) in [float ,int]:\n",
    "                value = np.array(value)\n",
    "            \n",
    "            self.requires_grad = requires_grad\n",
    "            \n",
    "            self.value = 1.0*value\n",
    "            self.grad = None\n",
    "\n",
    "            \n",
    "            if self.requires_grad:\n",
    "                self.grad = np.zeros_like(self.value)\n",
    "\n",
    "            self.shape = value.shape\n",
    "\n",
    "            self._backward = _backward\n",
    "            self.inputs = inputs\n",
    "\n",
    "            self.op = op\n",
    "            self.label = label\n",
    "\n",
    "        def topological_sort(self):\n",
    "            topo_order = []\n",
    "            visited = set()\n",
    "\n",
    "            def dfs(node):\n",
    "                if node not in visited:\n",
    "                    visited.add(node)\n",
    "                    for input in node.inputs:\n",
    "                        dfs(input)\n",
    "                    topo_order.append(node)\n",
    "\n",
    "            dfs(self)\n",
    "            return topo_order\n",
    "\n",
    "\n",
    "        def backward(self):\n",
    "            self.grad = np.array(1.0)\n",
    "\n",
    "            \n",
    "            topo_order = self.topological_sort()\n",
    "            \n",
    "            start_trace()  # added to trace memory used\n",
    "            mem_usage = [] # added to trace memory used\n",
    "\n",
    "            for node in reversed(topo_order):\n",
    "                node._backward()\n",
    "                mem_usage.append(snapshot_trace())\n",
    "            end_trace()\n",
    "            return mem_usage\n",
    "\n",
    "        ##########\n",
    "        # MATMUL #\n",
    "        ##########\n",
    "        def __matmul__(self,other):\n",
    "            \"\"\"\n",
    "            matrix multiplication between two MATRICES only\n",
    "            \"\"\"\n",
    "\n",
    "            assert(len(self.shape) == 2)\n",
    "            assert(len(other.shape) == 2)\n",
    "\n",
    "            output = ag.Tensor(np.matmul(self.value,other.value),\n",
    "                               inputs = [self,other],\n",
    "                               op=\"matmul\")\n",
    "            \n",
    "            def _backward():\n",
    "                # YOUR CODE HERE FOR initializing the grad\n",
    "                    \n",
    "                self.grad += np.matmul(output.grad, other.value.T)\n",
    "                other.grad += np.matmul(self.value.T, output.grad)\n",
    "\n",
    "                # YOUR CODE HERE FOR discarding the grad\n",
    "                return None\n",
    "            output._backward = _backward\n",
    "            return output\n",
    "        \n",
    "        def zero_grad(self):\n",
    "            self.grad = np.zeros_like(self.value)\n",
    "            return None\n",
    "        \n",
    "        def discard_grad(self):\n",
    "            self.grad = None\n",
    "            return None\n",
    "            \n",
    "        def __repr__(self) -> str:\n",
    "            return \"Value:\\n\"+self.value.__repr__() + \"\\nGrad:\\n\" + self.grad.__repr__()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebf187b1-9864-460b-b3a9-15ee853411b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tracemalloc\n",
    "import numpy as np\n",
    "# code adapted from \n",
    "# https://numpy.org/doc/2.0/reference/c-api/data_memory.html#example-of-memory-tracing-with-np-lib-tracemalloc-domain\n",
    "\n",
    "def start_trace():\n",
    "    tracemalloc.start()\n",
    "    return None\n",
    "\n",
    "def snapshot_trace():\n",
    "    snapshot = tracemalloc.take_snapshot()\n",
    "\n",
    "    # only keep track of the allocations by numpy\n",
    "    dom_filter = tracemalloc.DomainFilter(inclusive=True,\n",
    "                                          domain=np.lib.tracemalloc_domain)\n",
    "    \n",
    "    snapshot = snapshot.filter_traces([dom_filter])\n",
    "    top_stats = snapshot.statistics('traceback')\n",
    "\n",
    "    return top_stats\n",
    "    \n",
    "def end_trace():\n",
    "    \n",
    "    tracemalloc.clear_traces()\n",
    "    tracemalloc.stop()\n",
    "    return None\n",
    "    \n",
    "def print_trace_stats(stats):\n",
    "    mem_allocated = 0\n",
    "    for stat in stats:\n",
    "        mem_allocated += stat.size\n",
    "    print(f\"memory allocated: {mem_allocated//  1000000} MB\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d830c028-ac56-47d0-aa9d-7cf8ab99707c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'NoneType' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m reset_matmul_call_counter()\n\u001b[1;32m     28\u001b[0m l, mem_usage_forward \u001b[38;5;241m=\u001b[39m forward_traced_grad_discard(X,weights)\n\u001b[0;32m---> 29\u001b[0m mem_usage_backward \u001b[38;5;241m=\u001b[39m l\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(matmul_call_count)\n",
      "Cell \u001b[0;32mIn[1], line 105\u001b[0m, in \u001b[0;36mag.Tensor.backward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    102\u001b[0m mem_usage \u001b[38;5;241m=\u001b[39m [] \u001b[38;5;66;03m# added to trace memory used\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(topo_order):\n\u001b[0;32m--> 105\u001b[0m     node\u001b[38;5;241m.\u001b[39m_backward()\n\u001b[1;32m    106\u001b[0m     mem_usage\u001b[38;5;241m.\u001b[39mappend(snapshot_trace())\n\u001b[1;32m    107\u001b[0m end_trace()\n",
      "Cell \u001b[0;32mIn[1], line 39\u001b[0m, in \u001b[0;36mag.sum.<locals>._backward\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_backward\u001b[39m():\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;66;03m# YOUR CODE HERE FOR initialize the grad if they are none\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mgrad\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# YOUR CODE HERE FOR discarding the grad\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'NoneType' and 'float'"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "num_layers = 10\n",
    "num_samples = 4096\n",
    "dim_hidden = 1000\n",
    "\n",
    "weights = [ag.Tensor(0.02*np.random.randn(dim_hidden, dim_hidden), \n",
    "                     requires_grad = True) for _ in range(num_layers)]\n",
    "X = ag.Tensor(np.random.randn(num_samples, dim_hidden))\n",
    "\n",
    "def forward(x, weights):\n",
    "    for w in weights:\n",
    "        x = ag.matmul(x, w)\n",
    "    return ag.sum(x)\n",
    "\n",
    "def forward_traced_grad_discard(x, weights):\n",
    "    start_trace()\n",
    "    mem_usage = []\n",
    "    for w in weights:\n",
    "        x = ag.matmul(x, w)\n",
    "        mem_usage.append(snapshot_trace())\n",
    "    l = ag.sum(x)\n",
    "    mem_usage.append(snapshot_trace())\n",
    "    end_trace()\n",
    "    return l, mem_usage\n",
    "\n",
    "reset_matmul_call_counter()\n",
    "l, mem_usage_forward = forward_traced_grad_discard(X,weights)\n",
    "mem_usage_backward = l.backward()\n",
    "print(matmul_call_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5bb021-72c5-462a-b9d3-5eab279c6d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "print( f\"size of the numpy array {( dim_hidden * num_samples * 8 ) // 1000000} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7438092c-e857-4935-ace6-e2a6eed84c9d",
   "metadata": {},
   "source": [
    "# Sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f55f46f-dc5a-4497-9d78-c431b4dd6d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, trace_stats in enumerate(mem_usage_forward):\n",
    "    print(f\"layer {i}\")\n",
    "    print_trace_stats(trace_stats)\n",
    "# expected output\n",
    "# layer 0\n",
    "# memory allocated: 32 MB\n",
    "# layer 1\n",
    "# memory allocated: 65 MB\n",
    "# layer 2\n",
    "# memory allocated: 98 MB\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8289d1f-f2d6-4055-8915-34d0702b270a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, trace_stats in enumerate(mem_usage_backward):\n",
    "    print(f\"backward step {i}\")\n",
    "    print_trace_stats(trace_stats)\n",
    "\n",
    "# expected output\n",
    "# backward step 0\n",
    "# memory allocated: 32 MB\n",
    "# backward step 1\n",
    "# memory allocated: 32 MB\n",
    "# backward step 2\n",
    "# memory allocated: 32 MB\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d591eee0-883d-4d91-bf26-9e07494ed35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights[0].grad\n",
    "# EXPECTED OUTPUT\n",
    "\n",
    "# array([[ 0.12749943, -0.28241071, -0.05621888, ..., -0.02374291,\n",
    "#          0.27343724,  0.48198191],\n",
    "#        [ 0.25550796, -0.56594908, -0.11266225, ..., -0.04758063,\n",
    "#          0.5479663 ,  0.96588835],\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39052f8a-0d61-4226-9e23-f6b85bf84744",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
