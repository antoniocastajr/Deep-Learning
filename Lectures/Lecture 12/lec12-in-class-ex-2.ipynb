{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1eed5d57-839e-4105-b40b-bebff1c5ed99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "matmul_call_count = 0 # A counter to keep track of matmul calls\n",
    "\n",
    "def reset_matmul_call_counter():\n",
    "    global matmul_call_count\n",
    "    matmul_call_count = 0\n",
    "\n",
    "def matmul_counter_wrapper(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        global matmul_call_count\n",
    "        matmul_call_count += 1\n",
    "        return func(*args, **kwargs)\n",
    "    return wrapper\n",
    "\n",
    "# Replace np.matmul with the logged version\n",
    "np.matmul = matmul_counter_wrapper(np.matmul)\n",
    "\n",
    "class ag: # AutoGrad\n",
    "    \"\"\"\n",
    "    A barebone version of the AutoGrad library that we've been working with\n",
    "    it only supports \n",
    "    - matmul \n",
    "        - between two numpy matrices ONLY,\n",
    "            -i.e., arrays X such that len(X.shape) == 2\n",
    "    - sum \n",
    "        - \"axis = None\" ONLY\n",
    "            - so everything gets summed up\n",
    "    - add (entrywise)\n",
    "    \"\"\"\n",
    "\n",
    "    #################\n",
    "    # REDUCTIVE OPS #\n",
    "    #################\n",
    "    def sum(input):\n",
    "        output = ag.Tensor(np.sum(input.value), inputs = [input], op='sum')\n",
    "        def _backward():\n",
    "            if input.grad is None:\n",
    "                input.zero_grad()\n",
    "            input.grad += output.grad\n",
    "            if not output.requires_grad:\n",
    "                output.discard_grad()\n",
    "            return None\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "    ##########\n",
    "    # MATMUL #\n",
    "    ##########\n",
    "    def matmul(input1, input2):\n",
    "        return input1@input2\n",
    "\n",
    "    class Tensor: # Tensor with grads\n",
    "        def __init__(self,\n",
    "                     value,\n",
    "                     requires_grad=False,\n",
    "                     rematerializer = None, # None means don't rematerialize, KEEP\n",
    "                     op=\"\",\n",
    "                     _backward= lambda : None,\n",
    "                     inputs=[],\n",
    "                     label=\"\"):\n",
    "\n",
    "            if type(value) in [float ,int]:\n",
    "                value = np.array(value)\n",
    "            \n",
    "            self.requires_grad = requires_grad\n",
    "            self.rematerializer = rematerializer\n",
    "            \n",
    "            self.value = 1.0*value\n",
    "            self.grad = None\n",
    "            \n",
    "            if self.requires_grad:\n",
    "                self.grad = np.zeros_like(self.value)\n",
    "\n",
    "            self.shape = value.shape\n",
    "\n",
    "            self._backward = _backward\n",
    "            self.inputs = inputs\n",
    "\n",
    "            self.op = op\n",
    "            self.label = label\n",
    "\n",
    "        def topological_sort(self):\n",
    "            topo_order = []\n",
    "            visited = set()\n",
    "\n",
    "            def dfs(node):\n",
    "                if node not in visited:\n",
    "                    visited.add(node)\n",
    "                    for input in node.inputs:\n",
    "                        dfs(input)\n",
    "                    topo_order.append(node)\n",
    "\n",
    "            dfs(self)\n",
    "            return topo_order\n",
    "\n",
    "\n",
    "        def backward(self):\n",
    "            \"\"\"\n",
    "            memory tracing has been added\n",
    "            \"\"\"\n",
    "            self.grad = np.array(1.0)\n",
    "\n",
    "            \n",
    "            topo_order = self.topological_sort()\n",
    "            \n",
    "            start_trace()  # added to trace memory used\n",
    "            mem_usage = [] # added to trace memory used\n",
    "\n",
    "            reversed_topo_order = reversed(topo_order)\n",
    "            \n",
    "            for node in reversed(topo_order):\n",
    "                # YOUR CODE HERE FOR rematerializing \"input.value\", if it is none\n",
    "                        \n",
    "                node._backward()\n",
    "                \n",
    "                mem_usage.append(snapshot_trace())\n",
    "            end_trace()\n",
    "            return mem_usage\n",
    "\n",
    "        ##########\n",
    "        # MATMUL #\n",
    "        ##########\n",
    "        def __matmul__(self,other):\n",
    "            \"\"\"\n",
    "            matrix multiplication between two MATRICES only\n",
    "            \"\"\"\n",
    "\n",
    "            assert(len(self.shape) == 2)\n",
    "            assert(len(other.shape) == 2)\n",
    "\n",
    "            output = ag.Tensor(np.matmul(self.value,other.value),\n",
    "                               inputs = [self,other],\n",
    "                               op=\"matmul\")\n",
    "            \n",
    "            def _backward():\n",
    "                if self.grad is None:\n",
    "                    self.zero_grad()\n",
    "                if other.grad is None:\n",
    "                    other.zero_grad()\n",
    "                    \n",
    "                self.grad += np.matmul(output.grad, other.value.T)\n",
    "                other.grad += np.matmul(self.value.T, output.grad)\n",
    "\n",
    "                # YOUR CODE HERE FOR discarding activations for \"self\" and \"other\"\n",
    "                # hint: add a helper function to make it neater\n",
    "                # hint: see \"discard_value_if_has_rematerializer\" below\n",
    "                \n",
    "                if not output.requires_grad:\n",
    "                    output.discard_grad()\n",
    "                return None\n",
    "                \n",
    "            output._backward = _backward\n",
    "            # YOUR CODE HERE FOR discarding activations for \"self\" and \"other\"\n",
    "            \n",
    "            return output\n",
    "        \n",
    "        def zero_grad(self):\n",
    "            self.grad = np.zeros_like(self.value)\n",
    "            return None\n",
    "        \n",
    "        def discard_grad(self):\n",
    "            self.grad = None\n",
    "            return None\n",
    "            \n",
    "        def discard_value_if_has_rematerializer(self):\n",
    "            # related to the hint above\n",
    "            raise NotImplementedError\n",
    "            \n",
    "        def __repr__(self) -> str:\n",
    "            return \"Value:\\n\"+self.value.__repr__() + \"\\nGrad:\\n\" + self.grad.__repr__()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fa9c861-18f7-44d6-8928-ca24e6aad336",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tracemalloc\n",
    "import numpy as np\n",
    "# code adapted from \n",
    "# https://numpy.org/doc/2.0/reference/c-api/data_memory.html#example-of-memory-tracing-with-np-lib-tracemalloc-domain\n",
    "\n",
    "def start_trace():\n",
    "    tracemalloc.start()\n",
    "    return None\n",
    "\n",
    "def snapshot_trace():\n",
    "    snapshot = tracemalloc.take_snapshot()\n",
    "\n",
    "    # only keep track of the allocations by numpy\n",
    "    dom_filter = tracemalloc.DomainFilter(inclusive=True,\n",
    "                                          domain=np.lib.tracemalloc_domain)\n",
    "    \n",
    "    snapshot = snapshot.filter_traces([dom_filter])\n",
    "    top_stats = snapshot.statistics('traceback')\n",
    "\n",
    "    return top_stats\n",
    "    \n",
    "def end_trace():\n",
    "    \n",
    "    tracemalloc.clear_traces()\n",
    "    tracemalloc.stop()\n",
    "    return None\n",
    "    \n",
    "def print_trace_stats(stats):\n",
    "    mem_allocated = 0\n",
    "    for stat in stats:\n",
    "        mem_allocated += stat.size\n",
    "    print(f\"memory allocated: {mem_allocated//  1000000} MB\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db6461e9-f8fa-4507-85a5-099d8902a3d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "num_layers = 10\n",
    "num_samples = 4096\n",
    "dim_hidden = 1000\n",
    "\n",
    "weights = [ag.Tensor(0.02*np.random.randn(dim_hidden, dim_hidden), requires_grad = True) for _ in range(num_layers)]\n",
    "X = ag.Tensor(np.random.randn(num_samples, dim_hidden))\n",
    "\n",
    "\n",
    "def forward_traced_with_rematerializer(x, weights):\n",
    "    start_trace()\n",
    "    mem_usage = [] # tracing\n",
    "\n",
    "    checkpoints = [5] # these layers are checkpoints\n",
    "\n",
    "    farthest_checkpoint = 0 # this is the input data\n",
    "    x_at_farthest_checkpoint = x\n",
    "    \n",
    "    for i, w in enumerate(weights):\n",
    "        if i in checkpoints:\n",
    "            x = ag.matmul(x, w)\n",
    "            farthest_checkpoint = i\n",
    "            x_at_farthest_checkpoint = x\n",
    "        else:\n",
    "            def _rematerializer():\n",
    "                xval = x_at_farthest_checkpoint.value\n",
    "                for w in weights[farthest_checkpoint:(i+1)]:\n",
    "                    xval = np.matmul(xval,w.value)\n",
    "                return xval\n",
    "            x = ag.matmul(x, w)\n",
    "            x.rematerializer = _rematerializer\n",
    "            \n",
    "        \n",
    "        mem_usage.append(snapshot_trace()) # tracing\n",
    "        \n",
    "    l = ag.sum(x)\n",
    "    mem_usage.append(snapshot_trace()) # tracing\n",
    "    end_trace() # tracing\n",
    "    return l, mem_usage\n",
    "\n",
    "reset_matmul_call_counter()\n",
    "l, mem_usage_forward = forward_traced_with_rematerializer(X,weights)\n",
    "mem_usage_backward = l.backward()\n",
    "print(matmul_call_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a5bb021-72c5-462a-b9d3-5eab279c6d85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32000.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "( dim_hidden * num_samples * 8 ) / 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f19aa85-210e-43fe-bd13-94eb287ca4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 0\n",
      "memory allocated: 32 MB\n",
      "layer 1\n",
      "memory allocated: 65 MB\n",
      "layer 2\n",
      "memory allocated: 98 MB\n",
      "layer 3\n",
      "memory allocated: 131 MB\n",
      "layer 4\n",
      "memory allocated: 163 MB\n",
      "layer 5\n",
      "memory allocated: 196 MB\n",
      "layer 6\n",
      "memory allocated: 229 MB\n",
      "layer 7\n",
      "memory allocated: 262 MB\n",
      "layer 8\n",
      "memory allocated: 294 MB\n",
      "layer 9\n",
      "memory allocated: 327 MB\n",
      "layer 10\n",
      "memory allocated: 327 MB\n"
     ]
    }
   ],
   "source": [
    "for i, trace_stats in enumerate(mem_usage_forward):\n",
    "    print(f\"layer {i}\")\n",
    "    print_trace_stats(trace_stats)\n",
    "# expected output\n",
    "# layer 0\n",
    "# memory allocated: 32 MB\n",
    "# layer 1\n",
    "# memory allocated: 32 MB\n",
    "# ...\n",
    "# layer 5\n",
    "# memory allocated: 32 MB\n",
    "# layer 6\n",
    "# memory allocated: 65 MB\n",
    "# layer 7\n",
    "# memory allocated: 65 MB\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f55f46f-dc5a-4497-9d78-c431b4dd6d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backward step 0\n",
      "memory allocated: 32 MB\n",
      "backward step 1\n",
      "memory allocated: 32 MB\n",
      "backward step 2\n",
      "memory allocated: 32 MB\n",
      "backward step 3\n",
      "memory allocated: 32 MB\n",
      "backward step 4\n",
      "memory allocated: 32 MB\n",
      "backward step 5\n",
      "memory allocated: 32 MB\n",
      "backward step 6\n",
      "memory allocated: 32 MB\n",
      "backward step 7\n",
      "memory allocated: 32 MB\n",
      "backward step 8\n",
      "memory allocated: 32 MB\n",
      "backward step 9\n",
      "memory allocated: 32 MB\n",
      "backward step 10\n",
      "memory allocated: 32 MB\n",
      "backward step 11\n",
      "memory allocated: 32 MB\n",
      "backward step 12\n",
      "memory allocated: 32 MB\n",
      "backward step 13\n",
      "memory allocated: 32 MB\n",
      "backward step 14\n",
      "memory allocated: 32 MB\n",
      "backward step 15\n",
      "memory allocated: 32 MB\n",
      "backward step 16\n",
      "memory allocated: 32 MB\n",
      "backward step 17\n",
      "memory allocated: 32 MB\n",
      "backward step 18\n",
      "memory allocated: 32 MB\n",
      "backward step 19\n",
      "memory allocated: 32 MB\n",
      "backward step 20\n",
      "memory allocated: 32 MB\n",
      "backward step 21\n",
      "memory allocated: 32 MB\n"
     ]
    }
   ],
   "source": [
    "for i, trace_stats in enumerate(mem_usage_backward):\n",
    "    print(f\"backward step {i}\")\n",
    "    print_trace_stats(trace_stats)\n",
    "\n",
    "    \n",
    "# expected output\n",
    "# backward step 0\n",
    "# memory allocated: 65 MB\n",
    "# backward step 1\n",
    "# memory allocated: 65 MB\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8289d1f-f2d6-4055-8915-34d0702b270a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backward step 0, memory allocated: 31 Mb\n",
      "backward step 1, memory allocated: 31 Mb\n",
      "backward step 2, memory allocated: 31 Mb\n",
      "backward step 3, memory allocated: 31 Mb\n",
      "backward step 4, memory allocated: 31 Mb\n",
      "backward step 5, memory allocated: 31 Mb\n",
      "backward step 6, memory allocated: 31 Mb\n",
      "backward step 7, memory allocated: 31 Mb\n",
      "backward step 8, memory allocated: 31 Mb\n",
      "backward step 9, memory allocated: 31 Mb\n",
      "backward step 10, memory allocated: 31 Mb\n",
      "backward step 11, memory allocated: 31 Mb\n",
      "backward step 12, memory allocated: 31 Mb\n",
      "backward step 13, memory allocated: 31 Mb\n",
      "backward step 14, memory allocated: 31 Mb\n",
      "backward step 15, memory allocated: 31 Mb\n",
      "backward step 16, memory allocated: 31 Mb\n",
      "backward step 17, memory allocated: 31 Mb\n",
      "backward step 18, memory allocated: 31 Mb\n",
      "backward step 19, memory allocated: 31 Mb\n",
      "backward step 20, memory allocated: 31 Mb\n",
      "backward step 21, memory allocated: 31 Mb\n"
     ]
    }
   ],
   "source": [
    "for i, trace_stats in enumerate(mem_usage_backward):\n",
    "    mem_allocated = 0\n",
    "    for stat in trace_stats:\n",
    "        mem_allocated += stat.size\n",
    "    print(f\"backward step {i}, memory allocated: {mem_allocated// 2**20} Mb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d591eee0-883d-4d91-bf26-9e07494ed35a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.12749943, -0.28241071, -0.05621888, ..., -0.02374291,\n",
       "         0.27343724,  0.48198191],\n",
       "       [ 0.25550796, -0.56594908, -0.11266225, ..., -0.04758063,\n",
       "         0.5479663 ,  0.96588835],\n",
       "       [ 0.39728201, -0.87997801, -0.1751753 , ..., -0.07398176,\n",
       "         0.85201711,  1.50183213],\n",
       "       ...,\n",
       "       [-0.0141211 ,  0.03127818,  0.00622648, ...,  0.00262963,\n",
       "        -0.03028433, -0.05338153],\n",
       "       [ 0.16727405, -0.37051134, -0.07375688, ..., -0.03114974,\n",
       "         0.35873851,  0.63234061],\n",
       "       [-0.29072346,  0.64395127,  0.12818998, ...,  0.05413846,\n",
       "        -0.62349001, -1.09901236]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[0].grad\n",
    "# EXPECTED OUTPUT\n",
    "\n",
    "# array([[ 0.12749943, -0.28241071, -0.05621888, ..., -0.02374291,\n",
    "#          0.27343724,  0.48198191],\n",
    "#        [ 0.25550796, -0.56594908, -0.11266225, ..., -0.04758063,\n",
    "#          0.5479663 ,  0.96588835],\n",
    "# ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
