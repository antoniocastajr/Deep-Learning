{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b7d805a4-3b3c-4ba5-a53d-9eb2bffc4a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Broadcasted shape\n",
    "def broadcasted_shape(shape_X, shape_Y):\n",
    "\n",
    "    max_len = max(len(shape_X), len(shape_Y))\n",
    "    min_len = min(len(shape_X), len(shape_Y))\n",
    "    swapped = False\n",
    "\n",
    "    if len(shape_X) < max_len:\n",
    "        swapped = True\n",
    "        L, S = shape_Y, shape_X # L = long, S = short\n",
    "    else:\n",
    "        L, S = shape_X, shape_Y\n",
    "\n",
    "    L_rev = L[::-1]\n",
    "    S_rev = S[::-1]\n",
    "\n",
    "    result_shape = []\n",
    "    axes_L_expanded = []\n",
    "    axes_S_expanded = []\n",
    "\n",
    "    for i in range(min_len):\n",
    "        dim_L = L_rev[i]\n",
    "        dim_S = S_rev[i]\n",
    "        if dim_L == 1 and dim_S != 1:\n",
    "            axes_L_expanded.append(max_len  -1- i)\n",
    "        elif dim_L != 1 and dim_S == 1:\n",
    "            axes_S_expanded.append(max_len  -1- i)\n",
    "        if dim_L == 1 or dim_S == 1 or dim_L == dim_S:\n",
    "            result_shape.append(max(dim_L,dim_S))\n",
    "        else:\n",
    "            raise ValueError(f\"Shapes {shape_X} and {shape_Y} not broadcastable\")\n",
    "\n",
    "\n",
    "    result_shape += L_rev[(min_len):]\n",
    "\n",
    "    result_shape = tuple(result_shape[::-1])\n",
    "    axes_L_expanded = tuple(axes_L_expanded[::-1])\n",
    "    axes_S_expanded = tuple(axes_S_expanded[::-1])\n",
    "\n",
    "    if swapped:\n",
    "        return result_shape, axes_S_expanded, max_len - min_len,  axes_L_expanded, 0\n",
    "    else:\n",
    "        return result_shape,  axes_L_expanded, 0, axes_S_expanded, max_len - min_len\n",
    "\n",
    "def unbroadcast(arr, ax, pad):\n",
    "    return np.sum(np.sum(arr, axis = ax, keepdims = True), axis = tuple(range(pad)))\n",
    "\n",
    "class ag: # AutoGrad\n",
    "\n",
    "    #################\n",
    "    # ENTRYWISE OPS #\n",
    "    #################\n",
    "\n",
    "    def log(input):\n",
    "        output = ag.Tensor(np.log(input.value), inputs=[input], op=\"log\")\n",
    "        def _backward():\n",
    "            input.grad += output.grad / input.value\n",
    "            return None\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "    def exp(input):\n",
    "\n",
    "        output = ag.Tensor(np.exp(input.value), inputs=[input], op=\"exp\")\n",
    "\n",
    "        def _backward():\n",
    "            input.grad += output.grad * output.value\n",
    "            return None\n",
    "\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "    def relu(input):\n",
    "        output = ag.Tensor(np.maximum(0, input.value), inputs=[input], op=\"relu\")\n",
    "\n",
    "        def _backward():\n",
    "            input.grad += (input.value > 0)*output.grad\n",
    "            return None\n",
    "\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "    #################\n",
    "    # REDUCTIVE OPS #\n",
    "    #################\n",
    "    def sum(input,axis = None, keepdims = False):\n",
    "        output = ag.Tensor(np.sum(input.value, axis = axis, keepdims = keepdims), inputs = [input], op='sum')\n",
    "        def _backward():\n",
    "            if axis == None:\n",
    "                input.grad += output.grad\n",
    "            elif keepdims:\n",
    "                input.grad += np.sum(output.grad, axis = axis, keepdims=True)\n",
    "            else:\n",
    "                input.grad += np.expand_dims(output.grad, axis = axis)\n",
    "            return None\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "    def matmul(input1, input2):\n",
    "        return input1@input2\n",
    "    ###############\n",
    "    # SHAPING OPS #\n",
    "    ###############\n",
    "\n",
    "    def expand_dims(input, axis):\n",
    "        output = ag.Tensor(np.expand_dims(input.value,axis=axis), inputs = [input])\n",
    "        def _backward():\n",
    "            input.grad += np.squeeze(output.grad, axis = axis)\n",
    "            return None\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "    def moveaxis(input, source, destination):\n",
    "        output = ag.Tensor(np.moveaxis(input.value, source, destination), inputs=[input], op=\"moveaxis\")\n",
    "\n",
    "        def _backward():\n",
    "            input.grad += np.moveaxis(output.grad, source, destination)\n",
    "            return None\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "    class Tensor: # Tensor with grads\n",
    "        def __init__(self,\n",
    "                     value,\n",
    "                     op=\"\",\n",
    "                     _backward= lambda : None,\n",
    "                     inputs=[],\n",
    "                     label=\"\"):\n",
    "\n",
    "            if type(value) in [float ,int]:\n",
    "                value = np.array(value)\n",
    "            self.value = 1.0*value\n",
    "            self.grad = np.zeros_like(self.value)\n",
    "\n",
    "            self.shape = value.shape\n",
    "\n",
    "            self._backward = _backward\n",
    "            self.inputs = inputs\n",
    "\n",
    "            self.op = op\n",
    "            self.label = label\n",
    "\n",
    "        def topological_sort(self):\n",
    "            topo_order = []\n",
    "            visited = set()\n",
    "\n",
    "            def dfs(node):\n",
    "                if node not in visited:\n",
    "                    visited.add(node)\n",
    "                    for input in node.inputs:\n",
    "                        dfs(input)\n",
    "                    topo_order.append(node)\n",
    "\n",
    "            dfs(self)\n",
    "            return topo_order\n",
    "\n",
    "        def backward(self):\n",
    "            self.grad = np.array(1.0)\n",
    "\n",
    "            topo_order = self.topological_sort()\n",
    "\n",
    "            for node in reversed(topo_order):\n",
    "                node._backward()\n",
    "\n",
    "        def __add__(self, other):\n",
    "            if type(other) in [float, int]:\n",
    "                other = ag.Tensor(1.0*other)\n",
    "            result_shape, ax1, pad1, ax2, pad2 = broadcasted_shape(self.shape, other.shape)\n",
    "\n",
    "            output = ag.Tensor(self.value + other.value,\n",
    "                               inputs=[self, other], op=\"add\")\n",
    "            def _backward():\n",
    "                self.grad += unbroadcast(output.grad, ax1, pad1)\n",
    "                other.grad += unbroadcast(output.grad, ax2, pad2)\n",
    "\n",
    "            output._backward = _backward\n",
    "            return output\n",
    "\n",
    "        def __sub__(self,other):\n",
    "            return self + other*(-1)\n",
    "\n",
    "        def __neg__(self):\n",
    "            output = ag.Tensor(-self.value, inputs=[self], op=\"neg\")\n",
    "            def _backward():\n",
    "                self.grad -= output.grad\n",
    "                return None\n",
    "            output._backward = _backward\n",
    "            return output\n",
    "\n",
    "        def __mul__(self, other):\n",
    "            if type(other) in [float, int]:\n",
    "                other = ag.Tensor(1.0*other)\n",
    "            result_shape, ax1, pad1, ax2, pad2 = broadcasted_shape(self.shape, other.shape)\n",
    "\n",
    "            output = ag.Tensor(self.value * other.value,\n",
    "                               inputs=[self, other], op=\"mul\")\n",
    "            def _backward():\n",
    "                self.grad += unbroadcast(output.grad*other.value, ax1, pad1)\n",
    "                other.grad += unbroadcast(output.grad*self.value, ax2, pad2)\n",
    "\n",
    "            output._backward = _backward\n",
    "            return output\n",
    "\n",
    "        def __truediv__(self,other):\n",
    "            return self*(other**(-1))\n",
    "\n",
    "        def __radd__(self, other):\n",
    "            return self + other\n",
    "\n",
    "        def __rmul__(self, other):\n",
    "            return self * other\n",
    "\n",
    "        def __rsub__(self, other):\n",
    "            return (-self) + other\n",
    "\n",
    "        def __rtruediv__(self, other):\n",
    "            return ag.Tensor(other) / self\n",
    "\n",
    "        def __pow__(self, exponent): # exponent is just a python float\n",
    "            output = ag.Tensor(self.value ** exponent,\n",
    "                               inputs=[self],\n",
    "                               op=f\"pow({exponent})\")\n",
    "\n",
    "            def _backward():\n",
    "\n",
    "                self.grad += (exponent * self.value**(exponent-1)) * output.grad\n",
    "                return None\n",
    "\n",
    "            output._backward = _backward\n",
    "            return output\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            output = ag.Tensor(np.array(self.value[idx]),\n",
    "                               inputs = [self],\n",
    "                               op=f\"[...]\")\n",
    "            def _backward():\n",
    "                self.grad[idx] += output.grad # idx must not have repeats!\n",
    "                return None\n",
    "            output._backward = _backward\n",
    "            return output\n",
    "\n",
    "        def __matmul__(self,other):\n",
    "            \"\"\"\n",
    "            matrix multiplication between two tensors\n",
    "            where len(self.shape) > 1\n",
    "            in particular, we DISALLOW vector-matrix product\n",
    "            this includes the vector-vector product, in particular\n",
    "            \"\"\"\n",
    "\n",
    "            assert(len(self.shape) > 1)\n",
    "\n",
    "            output = ag.Tensor(np.matmul(self.value,other.value),\n",
    "                               inputs = [self,other],\n",
    "                               op=\"matmul\")\n",
    "\n",
    "            if len(other.value.shape) == 1:\n",
    "                def _backward():\n",
    "                    self.grad += np.matmul(output.grad[:,None], other.value[None,:])\n",
    "                    other.grad += np.sum(np.moveaxis(self.value,-1,0)*output.grad,\n",
    "                                         axis= tuple(range(1,len(self.shape))))\n",
    "                    # example\n",
    "                    # A = np.random.rand(3,4,5)\n",
    "                    # B = np.random.rand(3,4)\n",
    "                    # np.sum(np.moveaxis(A,-1,0)*B, axis= tuple(range(1,len(A.shape))))\n",
    "                    # the result has shape (5,)\n",
    "                    return None\n",
    "            else:\n",
    "                # if you reached here, you know that you have two tensors of rank â‰¥ 2\n",
    "                # now let's write\n",
    "                # self.shape = (subshape1, m,n)\n",
    "                # other.shape = (subshape2, n,p)\n",
    "                # note that we allow subshape1 and subshape 2 to be empty\n",
    "                # finalshape = broadcast(subshape1,subshape2)\n",
    "                # (self@other).shape =\n",
    "                # output.grad.shape = (finalshape, m, p)\n",
    "                result_shape, ax1, pad1, ax2, pad2 = broadcasted_shape(self.shape[:-2], other.shape[:-2])\n",
    "\n",
    "                def _backward():\n",
    "                    self.grad += unbroadcast(output.grad@np.moveaxis(other.value,-1,-2), ax1, pad1)\n",
    "                    other.grad += unbroadcast(np.moveaxis(self.value,-1,-2)@output.grad, ax2, pad2)\n",
    "                    return None\n",
    "            output._backward = _backward\n",
    "            return output\n",
    "\n",
    "        def __repr__(self) -> str:\n",
    "            return \"Value:\\n\"+self.value.__repr__() + \"\\nGrad:\\n\" + self.grad.__repr__()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "39d86bc0-68b2-49b7-9ba0-6bf5d241bbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class nn:\n",
    "\n",
    "    class BinaryCrossEntropyLoss:\n",
    "        def __call__(self, input, target):\n",
    "            N = target.value.shape[0]\n",
    "            return ag.sum( ag.log(1.0+ ag.exp(-input*target))) / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ea632303-a4d4-4cca-8b4f-f9c3e61cbdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleHeadAttention:\n",
    "    def __init__(self, n_features):\n",
    "        self.Wq = ag.Tensor(np.random.randn(n_features, n_features), label=\"Wq\") # this was W1\n",
    "        self.Wk = ag.Tensor(np.random.randn(n_features, n_features), label=\"Wk\") # this was W2\n",
    "        self.Wv = ag.Tensor(np.random.randn(n_features, n_features), label=\"Wv\") # this was w3\n",
    "    def __call__(self, Xin):\n",
    "        # Xin is a (n_samples, n_context, n_features) tensor\n",
    "        # Xout is *also* a (n_samples, n_context, n_features) tensor\n",
    "\n",
    "        \n",
    "        Queries = Xin @ self.Wq\n",
    "        Keys = Xin @ self.Wk\n",
    "        KQ = (Keys @ ag.moveaxis(Queries, 1,2))\n",
    "        expKQ = ag.exp(KQ)\n",
    "\n",
    "        # ADD SOMETHING HERE TO MAKE IT CAUSAL\n",
    "        # FOUR LINES\n",
    "\n",
    "        softmaxKQ = expKQ / ag.sum(expKQ, axis=1, keepdims=True)\n",
    "        Xout = ag.moveaxis(ag.moveaxis(Xin,1,2) @ softmaxKQ, 1,2) @ self.Wv\n",
    "        return Xout\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, n_features, n_hidden):\n",
    "        self.Wh = ag.Tensor(np.random.randn(n_features, n_hidden), label=\"Whidden\")\n",
    "        self.bh = ag.Tensor(np.random.randn(n_hidden), label=\"bhidden\")\n",
    "\n",
    "    def __call__(self, Xin):\n",
    "        hidden = ag.relu((Xin @ self.Wh) + self.bh)\n",
    "        return hidden\n",
    "\n",
    "class TransformerBlock:\n",
    "    def __init__(self, n_features, n_hidden):\n",
    "        self.att = SingleHeadAttention(n_features)\n",
    "        self.mlp = MLP(n_features, n_hidden)\n",
    "    def __call__(self, Xin):\n",
    "        return self.mlp(self.att(Xin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "19bc50f3-a38c-4d85-bcf7-6de21675419b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "n_context = 4\n",
    "n_features = 3\n",
    "n_samples = 5\n",
    "\n",
    "X1_np = np.array([[[-0.707, -0.707, 1.0],\n",
    "                   [0.963, -0.268, 1.0],\n",
    "                   [0.391, 0.92, -1.0],\n",
    "                   [0.899, 0.437, -1.0]],\n",
    "                  [[0.327, -0.945, 1.0],\n",
    "                   [0.3, -0.954, -1.0],\n",
    "                   [-0.485, -0.874, -1.0],\n",
    "                   [-0.694, 0.72, 1.0]],\n",
    "                  [[-0.938, -0.346, 1.0],\n",
    "                   [-0.742, 0.67, -1.0],\n",
    "                   [0.742, 0.67, -1.0],\n",
    "                   [0.322, 0.947, -1.0]]])  # (5, 4, 3)\n",
    "\n",
    "X2_np = np.array([[[-0.707, -0.707, 1.0],\n",
    "                   [0.963, -0.268, 1.0],\n",
    "                   [0.391, 0.92, -1.0],\n",
    "                   [1+0.899, 0.437, -1.0]], # A TINY EDIT\n",
    "                  [[0.327, -0.945, 1.0],\n",
    "                   [0.3, -0.954, -1.0],\n",
    "                   [-0.485, -0.874, -1.0],\n",
    "                   [-0.694, 0.72, 1.0]],\n",
    "                  [[-0.938, -0.346, 1.0],\n",
    "                   [-0.742, 0.67, -1.0],\n",
    "                   [0.742, 0.67, -1.0],\n",
    "                   [0.322, 0.947, -1.0]]])  # (5, 4, 3)\n",
    "\n",
    "y_np = np.array([-1.0, -1.0, 1.0, 1.0, -1.0]) # (5,)\n",
    "\n",
    "Wq_np = np.random.rand(3,3)\n",
    "Wk_np = np.random.rand(3,3)\n",
    "Wv_np = np.random.rand(3,3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "40c74b02-3239-4f65-9f66-d2ae1de0cd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "att = SingleHeadAttention(3)\n",
    "att.Wq.value = Wq_np\n",
    "att.Wk.value = Wk_np\n",
    "att.Wv.value = Wv_np\n",
    "\n",
    "X1 = ag.Tensor(X1_np)\n",
    "X2 = ag.Tensor(X2_np)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4327d3b7-409b-4668-a1fc-0966eca87fd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value:\n",
       "array([[ 0.43231678,  0.30688179, -0.93973424],\n",
       "       [ 1.00697546,  0.87703063,  0.43369102],\n",
       "       [ 0.20277333,  0.19984271, -0.12572647],\n",
       "       [ 0.04524706,  0.05245233,  0.28037472]])\n",
       "Grad:\n",
       "array([[0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att(X1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6f2cbefd-cb84-4c11-8f07-26a8f66e35ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value:\n",
       "array([[ 0.43231678,  0.30688179, -0.93973424],\n",
       "       [ 1.00697546,  0.87703063,  0.43369102],\n",
       "       [ 0.20277333,  0.19984271, -0.12572647],\n",
       "       [ 0.187932  ,  0.13987249,  0.88162416]])\n",
       "Grad:\n",
       "array([[0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att(X2)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a73a4e-2e4d-4074-b1d6-60753f606bc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
