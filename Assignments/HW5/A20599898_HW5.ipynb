{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mktDoGfQlaSF"
   },
   "source": [
    "# Homework 5\n",
    "\n",
    "\n",
    "## Autor: Antonio Casta√±ares A20599898\n",
    "\n",
    "\n",
    "See `hw5.pdf` for explanation of this homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "whfEj_UaOmhr"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Broadcasted shape\n",
    "def broadcasted_shape(shape_X, shape_Y):\n",
    "\n",
    "    max_len = max(len(shape_X), len(shape_Y))\n",
    "    min_len = min(len(shape_X), len(shape_Y))\n",
    "    swapped = False\n",
    "\n",
    "    if len(shape_X) < max_len:\n",
    "        swapped = True\n",
    "        L, S = shape_Y, shape_X # L = long, S = short\n",
    "    else:\n",
    "        L, S = shape_X, shape_Y\n",
    "\n",
    "    L_rev = L[::-1]\n",
    "    S_rev = S[::-1]\n",
    "\n",
    "    result_shape = []\n",
    "    axes_L_expanded = []\n",
    "    axes_S_expanded = []\n",
    "\n",
    "    for i in range(min_len):\n",
    "        dim_L = L_rev[i]\n",
    "        dim_S = S_rev[i]\n",
    "        if dim_L == 1 and dim_S != 1:\n",
    "            axes_L_expanded.append(max_len  -1- i)\n",
    "        elif dim_L != 1 and dim_S == 1:\n",
    "            axes_S_expanded.append(max_len  -1- i)\n",
    "        if dim_L == 1 or dim_S == 1 or dim_L == dim_S:\n",
    "            result_shape.append(max(dim_L,dim_S))\n",
    "        else:\n",
    "            raise ValueError(f\"Shapes {shape_X} and {shape_Y} not broadcastable\")\n",
    "\n",
    "\n",
    "    result_shape += L_rev[(min_len):]\n",
    "\n",
    "    result_shape = tuple(result_shape[::-1])\n",
    "    axes_L_expanded = tuple(axes_L_expanded[::-1])\n",
    "    axes_S_expanded = tuple(axes_S_expanded[::-1])\n",
    "\n",
    "    if swapped:\n",
    "        return result_shape, axes_S_expanded, max_len - min_len,  axes_L_expanded, 0\n",
    "    else:\n",
    "        return result_shape,  axes_L_expanded, 0, axes_S_expanded, max_len - min_len\n",
    "\n",
    "def unbroadcast(arr, ax, pad):\n",
    "    return np.sum(np.sum(arr, axis = ax, keepdims = True), axis = tuple(range(pad)))\n",
    "\n",
    "class ag: # AutoGrad\n",
    "\n",
    "    #################\n",
    "    # ENTRYWISE OPS #\n",
    "    #################\n",
    "\n",
    "    def log(input):\n",
    "        output = ag.Tensor(np.log(input.value), inputs=[input], op=\"log\")\n",
    "        def _backward():\n",
    "            input.grad += output.grad / input.value\n",
    "            return None\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "    def exp(input):\n",
    "\n",
    "        output = ag.Tensor(np.exp(input.value), inputs=[input], op=\"exp\")\n",
    "\n",
    "        def _backward():\n",
    "            input.grad += output.grad * output.value\n",
    "            return None\n",
    "\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "    def relu(input):\n",
    "        output = ag.Tensor(np.maximum(0, input.value), inputs=[input], op=\"relu\")\n",
    "\n",
    "        def _backward():\n",
    "            input.grad += (input.value > 0)*output.grad\n",
    "            return None\n",
    "\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "    #################\n",
    "    # REDUCTIVE OPS #\n",
    "    #################\n",
    "    def sum(input,axis = None, keepdims = False):\n",
    "        output = ag.Tensor(np.sum(input.value, axis = axis, keepdims = keepdims), inputs = [input], op='sum')\n",
    "        def _backward():\n",
    "            if axis == None:\n",
    "                input.grad += output.grad\n",
    "            elif keepdims:\n",
    "                input.grad += np.sum(output.grad, axis = axis, keepdims=True)\n",
    "            else:\n",
    "                input.grad += np.expand_dims(output.grad, axis = axis)\n",
    "            return None\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "    def matmul(input1, input2):\n",
    "        return input1@input2\n",
    "    ###############\n",
    "    # SHAPING OPS #\n",
    "    ###############\n",
    "\n",
    "    def expand_dims(input, axis):\n",
    "        output = ag.Tensor(np.expand_dims(input.value,axis=axis), inputs = [input])\n",
    "        def _backward():\n",
    "            input.grad += np.squeeze(output.grad, axis = axis)\n",
    "            return None\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "    def moveaxis(input, source, destination):\n",
    "        output = ag.Tensor(np.moveaxis(input.value, source, destination), inputs=[input], op=\"moveaxis\")\n",
    "\n",
    "        def _backward():\n",
    "            input.grad += np.moveaxis(output.grad, source, destination)\n",
    "            return None\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "    class Tensor: # Tensor with grads\n",
    "        def __init__(self,\n",
    "                     value,\n",
    "                     op=\"\",\n",
    "                     _backward= lambda : None,\n",
    "                     inputs=[],\n",
    "                     label=\"\"):\n",
    "\n",
    "            if type(value) in [float ,int]:\n",
    "                value = np.array(value)\n",
    "            self.value = 1.0*value\n",
    "            self.grad = np.zeros_like(self.value)\n",
    "\n",
    "            self.shape = value.shape\n",
    "\n",
    "            self._backward = _backward\n",
    "            self.inputs = inputs\n",
    "\n",
    "            self.op = op\n",
    "            self.label = label\n",
    "\n",
    "        def topological_sort(self):\n",
    "            topo_order = []\n",
    "            visited = set()\n",
    "\n",
    "            def dfs(node):\n",
    "                if node not in visited:\n",
    "                    visited.add(node)\n",
    "                    for input in node.inputs:\n",
    "                        dfs(input)\n",
    "                    topo_order.append(node)\n",
    "\n",
    "            dfs(self)\n",
    "            return topo_order\n",
    "\n",
    "        def backward(self):\n",
    "            self.grad = np.array(1.0)\n",
    "\n",
    "            topo_order = self.topological_sort()\n",
    "\n",
    "            for node in reversed(topo_order):\n",
    "                node._backward()\n",
    "\n",
    "        def __add__(self, other):\n",
    "            if type(other) in [float, int]:\n",
    "                other = ag.Tensor(1.0*other)\n",
    "            result_shape, ax1, pad1, ax2, pad2 = broadcasted_shape(self.shape, other.shape)\n",
    "\n",
    "            output = ag.Tensor(self.value + other.value,\n",
    "                               inputs=[self, other], op=\"add\")\n",
    "            def _backward():\n",
    "                self.grad += unbroadcast(output.grad, ax1, pad1)\n",
    "                other.grad += unbroadcast(output.grad, ax2, pad2)\n",
    "\n",
    "            output._backward = _backward\n",
    "            return output\n",
    "\n",
    "        def __sub__(self,other):\n",
    "            return self + other*(-1)\n",
    "\n",
    "        def __neg__(self):\n",
    "            output = ag.Tensor(-self.value, inputs=[self], op=\"neg\")\n",
    "            def _backward():\n",
    "                self.grad -= output.grad\n",
    "                return None\n",
    "            output._backward = _backward\n",
    "            return output\n",
    "\n",
    "        def __mul__(self, other):\n",
    "            if type(other) in [float, int]:\n",
    "                other = ag.Tensor(1.0*other)\n",
    "            result_shape, ax1, pad1, ax2, pad2 = broadcasted_shape(self.shape, other.shape)\n",
    "\n",
    "            output = ag.Tensor(self.value * other.value,\n",
    "                               inputs=[self, other], op=\"mul\")\n",
    "            def _backward():\n",
    "                self.grad += unbroadcast(output.grad*other.value, ax1, pad1)\n",
    "                other.grad += unbroadcast(output.grad*self.value, ax2, pad2)\n",
    "\n",
    "            output._backward = _backward\n",
    "            return output\n",
    "\n",
    "        def __truediv__(self,other):\n",
    "            return self*(other**(-1))\n",
    "\n",
    "        def __radd__(self, other):\n",
    "            return self + other\n",
    "\n",
    "        def __rmul__(self, other):\n",
    "            return self * other\n",
    "\n",
    "        def __rsub__(self, other):\n",
    "            return (-self) + other\n",
    "\n",
    "        def __rtruediv__(self, other):\n",
    "            return ag.Tensor(other) / self\n",
    "\n",
    "        def __pow__(self, exponent): # exponent is just a python float\n",
    "            output = ag.Tensor(self.value ** exponent,\n",
    "                               inputs=[self],\n",
    "                               op=f\"pow({exponent})\")\n",
    "\n",
    "            def _backward():\n",
    "\n",
    "                self.grad += (exponent * self.value**(exponent-1)) * output.grad\n",
    "                return None\n",
    "\n",
    "            output._backward = _backward\n",
    "            return output\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            output = ag.Tensor(np.array(self.value[idx]),\n",
    "                               inputs = [self],\n",
    "                               op=f\"[...]\")\n",
    "            def _backward():\n",
    "                self.grad[idx] += output.grad # idx must not have repeats!\n",
    "                return None\n",
    "            output._backward = _backward\n",
    "            return output\n",
    "\n",
    "        def __matmul__(self,other):\n",
    "            \"\"\"\n",
    "            matrix multiplication between two tensors\n",
    "            where len(self.shape) > 1\n",
    "            in particular, we DISALLOW vector-matrix product\n",
    "            this includes the vector-vector product, in particular\n",
    "            \"\"\"\n",
    "\n",
    "            assert(len(self.shape) > 1)\n",
    "\n",
    "            output = ag.Tensor(np.matmul(self.value,other.value),\n",
    "                               inputs = [self,other],\n",
    "                               op=\"matmul\")\n",
    "\n",
    "            if len(other.value.shape) == 1:\n",
    "                def _backward():\n",
    "                    self.grad += np.matmul(output.grad[:,None], other.value[None,:])\n",
    "                    other.grad += np.sum(np.moveaxis(self.value,-1,0)*output.grad,\n",
    "                                         axis= tuple(range(1,len(self.shape))))\n",
    "                    # example\n",
    "                    # A = np.random.rand(3,4,5)\n",
    "                    # B = np.random.rand(3,4)\n",
    "                    # np.sum(np.moveaxis(A,-1,0)*B, axis= tuple(range(1,len(A.shape))))\n",
    "                    # the result has shape (5,)\n",
    "                    return None\n",
    "            else:\n",
    "                # if you reached here, you know that you have two tensors of rank ‚â• 2\n",
    "                # now let's write\n",
    "                # self.shape = (subshape1, m,n)\n",
    "                # other.shape = (subshape2, n,p)\n",
    "                # note that we allow subshape1 and subshape 2 to be empty\n",
    "                # finalshape = broadcast(subshape1,subshape2)\n",
    "                # (self@other).shape =\n",
    "                # output.grad.shape = (finalshape, m, p)\n",
    "                result_shape, ax1, pad1, ax2, pad2 = broadcasted_shape(self.shape[:-2], other.shape[:-2])\n",
    "\n",
    "                def _backward():\n",
    "                    self.grad += unbroadcast(output.grad@np.moveaxis(other.value,-1,-2), ax1, pad1)\n",
    "                    other.grad += unbroadcast(np.moveaxis(self.value,-1,-2)@output.grad, ax2, pad2)\n",
    "                    return None\n",
    "            output._backward = _backward\n",
    "            return output\n",
    "\n",
    "        def __repr__(self) -> str:\n",
    "            return \"Value:\\n\"+self.value.__repr__() + \"\\nGrad:\\n\" + self.grad.__repr__()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "MqHIaP11QwWY"
   },
   "outputs": [],
   "source": [
    "class nn:\n",
    "    class BinaryCrossEntropyLoss:\n",
    "        def __call__(self, input, target):\n",
    "            N = target.value.shape[0]\n",
    "            return ag.sum( ag.log(1.0+ ag.exp(-input*target))) / N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1NcPzQey-U_N"
   },
   "source": [
    "## SingleheadAttention in `ag.Tensor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "1jf3cSgaEEVQ"
   },
   "outputs": [],
   "source": [
    "class SingleHeadAttention:\n",
    "    def __init__(self, n_features):\n",
    "        self.Wq = ag.Tensor(np.random.randn(n_features, n_features), label=\"Wq\") # this was W1\n",
    "        self.Wk = ag.Tensor(np.random.randn(n_features, n_features), label=\"Wk\") # this was W2\n",
    "        self.Wv = ag.Tensor(np.random.randn(n_features, n_features), label=\"Wv\") # this was w3\n",
    "    def __call__(self, Xin):\n",
    "        # Xin is a (n_samples, n_context, n_features) tensor\n",
    "        # Xout is *also* a (n_samples, n_context, n_features) tensor\n",
    "        Queries = Xin @ self.Wq\n",
    "        Keys = Xin @ self.Wk\n",
    "        KQ = (Keys @ ag.moveaxis(Queries, 1,2))\n",
    "        expKQ = ag.exp(KQ)\n",
    "        softmaxKQ = expKQ / ag.sum(expKQ, axis=1, keepdims=True)\n",
    "        Xout = ag.moveaxis(ag.moveaxis(Xin,1,2) @ softmaxKQ, 1,2) @ self.Wv\n",
    "        return Xout\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, n_features, n_hidden):\n",
    "        self.Wh = ag.Tensor(np.random.randn(n_features, n_hidden), label=\"Whidden\")\n",
    "        self.bh = ag.Tensor(np.random.randn(n_hidden), label=\"bhidden\")\n",
    "        self.wo = ag.Tensor(np.random.randn(n_hidden, n_features), label=\"Wout\")\n",
    "        self.bo = ag.Tensor(np.random.randn(n_features), label=\"bout\")\n",
    "\n",
    "    def __call__(self, Xin):\n",
    "        hidden = ag.relu((Xin @ self.Wh) + self.bh)\n",
    "        return hidden @ self.wo + self.bo\n",
    "\n",
    "class TransformerBlock:\n",
    "    def __init__(self, n_features, n_hidden):\n",
    "        self.att = SingleHeadAttention(n_features)\n",
    "        self.mlp = MLP(n_features, n_hidden)\n",
    "    def __call__(self, Xin):\n",
    "        return self.mlp(self.att(Xin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "nFS7ncY9PRrq"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "n_context = 4\n",
    "n_features = 3\n",
    "n_samples = 5\n",
    "\n",
    "X_np = np.array([[[-0.707, -0.707, 1.0],\n",
    "                   [0.963, -0.268, 1.0],\n",
    "                   [0.391, 0.92, -1.0],\n",
    "                   [0.899, 0.437, -1.0]],\n",
    "                  [[0.327, -0.945, 1.0],\n",
    "                   [0.3, -0.954, -1.0],\n",
    "                   [-0.485, -0.874, -1.0],\n",
    "                   [-0.694, 0.72, 1.0]],\n",
    "                  [[-0.309, 0.951, -1.0],\n",
    "                   [-0.951, 0.31, 1.0],\n",
    "                   [-0.9, -0.437, 1.0],\n",
    "                   [-0.013, -1.0, -1.0]],\n",
    "                  [[0.829, -0.559, -1.0],\n",
    "                   [-0.856, 0.518, 1.0],\n",
    "                   [-0.2, -0.98, -1.0],\n",
    "                   [-0.842, -0.539, 1.0]],\n",
    "                  [[-0.938, -0.346, 1.0],\n",
    "                   [-0.742, 0.67, -1.0],\n",
    "                   [0.742, 0.67, -1.0],\n",
    "                   [0.322, 0.947, -1.0]]])  # (5, 4, 3)\n",
    "\n",
    "y_np = np.array([-1.0, -1.0, 1.0, 1.0, -1.0]) # (5,)\n",
    "\n",
    "Wq_np = np.array([[0.74,   0.529,  0],\n",
    "                  [-0.589, 0.189,  0],\n",
    "                  [-0.759, -0.933, 0]])#  has shape (3,3)\n",
    "\n",
    "Wk_np = np.array([[0.504,   0.651, 0],\n",
    "                  [-0.319, -0.848, 0],\n",
    "                  [0.606,  -2.018, 0]]) #  has shape (3,3)\n",
    "\n",
    "Wv_np = np.array([[2.707, 0, 0],\n",
    "                  [0.628, 0, 0],\n",
    "                  [0.908, 0, 0]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MyxGfs8MWXp1"
   },
   "source": [
    "## [5] points - Problem 1a\n",
    "\n",
    "Given the data from the previous block, solve Problem 4 from Homework 4 using `SingleHeadAttention`. Starter code is given in the block below.\n",
    "\n",
    "Hint 1: recall from homework 4 we had\n",
    "\n",
    "`W1_raw = [[0.74, 0.529], [-0.589, 0.189], [-0.759, -0.933]]`\n",
    "\n",
    "`W2_raw = [[0.504, 0.651], [-0.319, -0.848], [0.606, -2.018]]`\n",
    "\n",
    "`w3_raw = [2.707, 0.628, 0.908]`\n",
    "\n",
    "Hint 2:\n",
    "\n",
    " Note that\n",
    "\n",
    "- `W1_raw` is the first two columns of `Wq_np`\n",
    "\n",
    "- `W2_raw` is the first two columns of `Wk_np`\n",
    "\n",
    "- `w3_raw` is the first column of `Wv_np`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VEussSB2HS6t",
    "outputId": "306997d1-9911-4c39-bc64-90c2936ae94d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dJdWQ\n",
      " [[ 0.02673784  0.01136829]\n",
      " [ 0.04348804  0.05114747]\n",
      " [-0.06112444 -0.05294486]]\n",
      "dJdW2\n",
      " [[ 0.05486582  0.13401244]\n",
      " [-0.01348159 -0.00905539]\n",
      " [ 0.01421121  0.01478831]]\n",
      "dJdw3\n",
      " [ 0.29006619  0.31213455 -0.22591685]\n"
     ]
    }
   ],
   "source": [
    "att = SingleHeadAttention(3)\n",
    "att.Wq.value = Wq_np\n",
    "att.Wk.value = Wk_np\n",
    "att.Wv.value = Wv_np\n",
    "\n",
    "X = ag.Tensor(X_np)\n",
    "y = ag.Tensor(y_np)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "#print(ag.sum(att(X)[2,3,0]).value)  # The last row of each sample is the output because its the sum of the previous weights\n",
    "#print(att(X)[2,3,0].value)\n",
    "loss = nn.BinaryCrossEntropyLoss()(att(X)[:,3,0], y) # Output and target are both (5,)\n",
    "loss.backward() \n",
    "\n",
    "\n",
    "print(\"dJdWQ\\n\", att.Wq.grad[:,:2])\n",
    "print(\"dJdW2\\n\", att.Wk.grad[:,:2])\n",
    "print(\"dJdw3\\n\", att.Wv.grad[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "6PBeqXdsI20h"
   },
   "outputs": [],
   "source": [
    "# dJdW1\n",
    "# [[ 0.02673784,  0.01136829],\n",
    "#  [ 0.04348804,  0.05114747],\n",
    "#  [-0.06112444, -0.05294486]]\n",
    "# dJdW2\n",
    "# [[ 0.05486582,  0.13401244],\n",
    "#  [-0.01348159, -0.00905539],\n",
    "#  [ 0.01421121,  0.01478831]]\n",
    "# dJdw3\n",
    "# [ 0.29006619,  0.31213455, -0.22591685]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
